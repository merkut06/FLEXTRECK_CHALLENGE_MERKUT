{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b677afff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b677afff",
        "outputId": "08e37958-1fb7-403f-fd34-0c0639c5a363"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n",
            "GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# 📦 Import Required Libraries\n",
        "# ===============================\n",
        "\n",
        "import os                # File and directory operations\n",
        "import random            # For generating random numbers\n",
        "import numpy as np       # Numerical operations\n",
        "import pandas as pd      # Data manipulation and analysis\n",
        "from datetime import datetime  # Handling date and time data\n",
        "\n",
        "# Deep learning imports (TensorFlow + Keras)\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Machine learning utilities (data scaling, metrics, splitting)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import joblib  # For saving and loading preprocessing models (e.g., scalers)\n",
        "\n",
        "# ===============================\n",
        "# 🎲 Set Random Seed for Reproducibility\n",
        "# ===============================\n",
        "\n",
        "SEED = 42  # Fix the seed value so results are consistent across runs\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# This ensures that TensorFlow operations are deterministic (reproducible)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# ===============================\n",
        "# ⚙️ Environment Information\n",
        "# ===============================\n",
        "\n",
        "# Print TensorFlow version (helps ensure compatibility)\n",
        "print(tf.__version__)\n",
        "\n",
        "# Try to detect available GPUs\n",
        "try:\n",
        "    print('GPU:', tf.config.list_physical_devices('GPU'))\n",
        "except Exception as e:\n",
        "    print('GPU info error:', e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zcEm886j5Dvb",
      "metadata": {
        "id": "zcEm886j5Dvb"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 🖥️ Pandas Display Options\n",
        "# ===============================\n",
        "\n",
        "# Show all columns when displaying a DataFrame\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Show all rows when displaying a DataFrame\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Set the display width for better readability in wide DataFrames\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "# Set maximum column width to display long text without truncation\n",
        "pd.set_option('display.max_colwidth', 1000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de2f7e08",
      "metadata": {
        "id": "de2f7e08"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 📁 Define File Paths & Directories\n",
        "# ===============================\n",
        "\n",
        "BASE_DIR = ''  # Base directory for your dataset and models\n",
        "\n",
        "# Paths to CSV files\n",
        "TRAIN_CSV = os.path.join(BASE_DIR, '')  # Training data CSV path\n",
        "TEST_CSV = os.path.join(BASE_DIR, '')   # Test data CSV path\n",
        "PREDICTIONS_CSV = os.path.join('predictions.csv')  # Where to save predictions\n",
        "\n",
        "# Directory to save trained models\n",
        "MODEL_DIR = os.path.join(BASE_DIR, '')\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# ===============================\n",
        "# 🕒 Column Names in Dataset\n",
        "# ===============================\n",
        "\n",
        "TIME_COL = 'Timestamp_Local'            # Column containing timestamp of each record\n",
        "SITE_COL = 'Site'                        # Column indicating the building/site\n",
        "FLAG_COL = 'Demand_Response_Flag'       # Target flag for demand response events (-1,0,1)\n",
        "TARGET_COL = 'Demand_Response_Capacity_kW'  # Target numeric column (capacity in kW)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tuJ__kp2HNbA",
      "metadata": {
        "id": "tuJ__kp2HNbA"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 📂 Load Training and Test Data\n",
        "# ===============================\n",
        "\n",
        "# Load the training dataset from the specified CSV file\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "\n",
        "# Load the test dataset from the specified CSV file\n",
        "test_df = pd.read_csv(TEST_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HTpl7Fl8HOaR",
      "metadata": {
        "collapsed": true,
        "id": "HTpl7Fl8HOaR"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 📊 Inspect Data Shapes and Columns\n",
        "# ===============================\n",
        "\n",
        "# Print the number of rows and columns in the training and test datasets\n",
        "print('[INFO] Train shape:', train_df.shape)\n",
        "print('[INFO] Test shape:', test_df.shape)\n",
        "\n",
        "# Print the column names for both datasets\n",
        "print('[INFO] Train columns:', list(train_df.columns))\n",
        "print('[INFO] Test columns:', list(test_df.columns))\n",
        "\n",
        "# Display the first 10 rows of each dataset to get a quick look at the data\n",
        "print('[INFO] Train head (first 10 rows):')\n",
        "print(train_df.head(10))\n",
        "\n",
        "print('[INFO] Test head (first 10 rows):')\n",
        "print(test_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-SyIba4aNXb-",
      "metadata": {
        "id": "-SyIba4aNXb-"
      },
      "outputs": [],
      "source": [
        "# ===============================\n",
        "# 🛠️ Feature Engineering: Create Enhanced Features\n",
        "# ===============================\n",
        "\n",
        "def create_enhanced_features(df):\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert timestamp to datetime object\n",
        "    df['Timestamp_Local'] = pd.to_datetime(df['Timestamp_Local'])\n",
        "\n",
        "    # Extract basic temporal features\n",
        "    df['hour'] = df['Timestamp_Local'].dt.hour\n",
        "    df['month'] = df['Timestamp_Local'].dt.month\n",
        "    df['weekday'] = df['Timestamp_Local'].dt.weekday  # 0=Monday, 6=Sunday\n",
        "\n",
        "    # Encode cyclic features using sine and cosine transformations\n",
        "    hour_rad = 2 * np.pi * df['hour'] / 24\n",
        "    df['hour_sin'] = np.sin(hour_rad)\n",
        "    df['hour_cos'] = np.cos(hour_rad)\n",
        "\n",
        "    month_rad = 2 * np.pi * (df['month'] - 1) / 12\n",
        "    df['month_sin'] = np.sin(month_rad)\n",
        "    df['month_cos'] = np.cos(month_rad)\n",
        "\n",
        "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
        "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
        "\n",
        "    # Binary flags for specific months\n",
        "    allowed_months = {1, 2, 6, 7, 8, 12}\n",
        "    df['is_allowed_month'] = df['month'].isin(allowed_months).astype(int)\n",
        "\n",
        "    # Binary flags for each hour from 7 AM to 19 PM\n",
        "    for h in range(7, 20):\n",
        "        df[f'is_{h}'] = (df['hour'] == h).astype(int)\n",
        "\n",
        "    # Binary flags for specific months\n",
        "    df['is_january'] = (df['month'] == 1).astype(int)\n",
        "    df['is_february'] = (df['month'] == 2).astype(int)\n",
        "    df['is_june'] = (df['month'] == 6).astype(int)\n",
        "    df['is_july'] = (df['month'] == 7).astype(int)\n",
        "    df['is_august'] = (df['month'] == 8).astype(int)\n",
        "    df['is_december'] = (df['month'] == 12).astype(int)\n",
        "\n",
        "    # Morning and afternoon flags\n",
        "    df['is_morning'] = ((df['hour'] >= 10) & (df['hour'] <= 11)).astype(int)\n",
        "    df['is_afternoon'] = ((df['hour'] >= 12) & (df['hour'] <= 17)).astype(int)\n",
        "\n",
        "    # Drop original hour, weekday, month columns after encoding\n",
        "    df = df.drop(columns=['hour','weekday','month'])\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply enhanced feature creation to train and test sets\n",
        "train_df = create_enhanced_features(train_df)\n",
        "test_df = create_enhanced_features(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cY9jqoe9pOuV",
      "metadata": {
        "collapsed": true,
        "id": "cY9jqoe9pOuV"
      },
      "outputs": [],
      "source": [
        "# ======================================================\n",
        "# 🕰️ Feature Engineering: Time-of-Day & Past Days Features\n",
        "# (comments added; code logic unchanged)\n",
        "# ======================================================\n",
        "\n",
        "def create_timeofday_pastdays_features(\n",
        "    df,\n",
        "    timestamp_col='Timestamp_Local',\n",
        "    power_col='Building_Power_kW',\n",
        "    temp_col='Dry_Bulb_Temperature_C',\n",
        "    windows=(1,2,3,4,5,6,7,8,9,10,11,12,13,14),\n",
        "    min_periods=1,\n",
        "    time_format='%H:%M:%S'\n",
        "    ):\n",
        "    \"\"\"\n",
        "    For each original timestamp, compute summaries of the same time-of-day over the\n",
        "    previous N days (exclude current day). Produces features for both power_col and temp_col.\n",
        "    Returns: (df_with_features, daily_agg_df)\n",
        "    \"\"\"\n",
        "    # make a copy to avoid modifying caller's dataframe\n",
        "    df = df.copy()\n",
        "\n",
        "    # ----------------------\n",
        "    # helper columns\n",
        "    # ----------------------\n",
        "    # 'date' is midnight-normalized timestamp for grouping by calendar day\n",
        "    df['date'] = df[timestamp_col].dt.normalize()        # midnight timestamp for the date\n",
        "    # 'time_of_day' is a string representation of the time portion used for aligning\n",
        "    # observations across different days at the same clock time\n",
        "    df['time_of_day'] = df[timestamp_col].dt.strftime(time_format)\n",
        "\n",
        "\n",
        "    # NOTE: the function arguments name power_col and temp_col but below the\n",
        "    # code reassigns power_col/temp_col to the literal column names. This keeps\n",
        "    # the subsequent code consistent with the original implementation.\n",
        "    power_col = 'Building_Power_kW'\n",
        "\n",
        "    # ----------------------\n",
        "    # trailing-power deltas (differences with previous timestamps)\n",
        "    # each shift corresponds to the prior interval (assumes regular sampling)\n",
        "    # ----------------------\n",
        "    df['trailing_15m_pow_delta'] = df[power_col] - df[power_col].shift(1)\n",
        "    df['trailing_30m_pow_delta'] = df[power_col] - df[power_col].shift(2)\n",
        "    df['trailing_45m_pow_delta'] = df[power_col] - df[power_col].shift(3)\n",
        "    df['trailing_60m_pow_delta'] = df[power_col] - df[power_col].shift(4)\n",
        "    df['trailing_1.25_hours_pow_delta'] = df[power_col] - df[power_col].shift(5)\n",
        "    df['trailing_1.5_hours_pow_delta'] = df[power_col] - df[power_col].shift(6)\n",
        "    df['trailing_1.75_hours_pow_delta'] = df[power_col] - df[power_col].shift(7)\n",
        "    df['trailing_2_hours_pow_delta'] = df[power_col] - df[power_col].shift(8)\n",
        "    df['trailing_2.25_hours_pow_delta'] = df[power_col] - df[power_col].shift(9)\n",
        "    df['trailing_2.5_hours_pow_delta'] = df[power_col] - df[power_col].shift(10)\n",
        "    df['trailing_2.75_hours_pow_delta'] = df[power_col] - df[power_col].shift(11)\n",
        "    df['trailing_3_hours_pow_delta'] = df[power_col] - df[power_col].shift(12)\n",
        "    df['trailing_3.25_hours_pow_delta'] = df[power_col] - df[power_col].shift(13)\n",
        "    df['trailing_3.5_hours_pow_delta'] = df[power_col] - df[power_col].shift(14)\n",
        "    df['trailing_3.75_hours_pow_delta'] = df[power_col] - df[power_col].shift(15)\n",
        "    df['trailing_4_hours_pow_delta'] = df[power_col] - df[power_col].shift(16)\n",
        "    df['trailing_4.25_hours_pow_delta'] = df[power_col] - df[power_col].shift(17)\n",
        "    df['trailing_4.5_hours_pow_delta'] = df[power_col] - df[power_col].shift(18)\n",
        "    df['trailing_4.75_hours_pow_delta'] = df[power_col] - df[power_col].shift(19)\n",
        "    df['trailing_5_hours_pow_delta'] = df[power_col] - df[power_col].shift(20)\n",
        "    df['trailing_5.25_hours_pow_delta'] = df[power_col] - df[power_col].shift(21)\n",
        "    df['trailing_5.5_hours_pow_delta'] = df[power_col] - df[power_col].shift(22)\n",
        "    df['trailing_5.75_hours_pow_delta'] = df[power_col] - df[power_col].shift(23)\n",
        "    df['trailing_6_hours_pow_delta'] = df[power_col] - df[power_col].shift(24)\n",
        "    df['trailing_7_hours_pow_delta'] = df[power_col] - df[power_col].shift(28)\n",
        "    df['trailing_8_hours_pow_delta'] = df[power_col] - df[power_col].shift(32)\n",
        "    df['trailing_9_hours_pow_delta'] = df[power_col] - df[power_col].shift(36)\n",
        "    df['trailing_10_hours_pow_delta'] = df[power_col] - df[power_col].shift(40)\n",
        "    df['trailing_11_hours_pow_delta'] = df[power_col] - df[power_col].shift(44)\n",
        "    df['trailing_12_hours_pow_delta'] = df[power_col] - df[power_col].shift(48)\n",
        "\n",
        "    # ----------------------\n",
        "    # rolling aggregates for short windows (mean/min/max)\n",
        "    # windows list maps sampling-window length -> human-readable label\n",
        "    # min_periods=1 allows early rows to have aggregates computed\n",
        "    # ----------------------\n",
        "    for window, label in [(2,'0.5'),(4, '1h'),(6,'1.5h'), (8, '2h'),(10,'2.5h'), (12,'3h'), (16, '4h'),(48,'12h')]:\n",
        "        df[f'rolling_{label}_avg_pow'] = df[power_col].rolling(window, min_periods=1).mean()\n",
        "        df[f'rolling_{label}_min_pow'] = df[power_col].rolling(window, min_periods=1).min()\n",
        "        df[f'rolling_{label}_max_pow'] = df[power_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "\n",
        "\n",
        "    # ----------------------\n",
        "    # rolling higher-order moments (skewness, kurtosis) for power\n",
        "    # windows chosen to match earlier labels (e.g. 8 corresponds to ~2 hours)\n",
        "    # ----------------------\n",
        "    df['trailing_2h_skew_pow'] = df[power_col].rolling(window=8, min_periods=1).skew()\n",
        "    df['trailing_2h_kurt_pow'] = df[power_col].rolling(window=8, min_periods=1).kurt()\n",
        "    df['trailing_2.25h_skew_pow'] = df[power_col].rolling(window=9, min_periods=1).skew()\n",
        "    df['trailing_2.25h_kurt_pow'] = df[power_col].rolling(window=9, min_periods=1).kurt()\n",
        "    df['trailing_2.5h_skew_pow'] = df[power_col].rolling(window=10, min_periods=1).skew()\n",
        "    df['trailing_2.5h_kurt_pow'] = df[power_col].rolling(window=10, min_periods=1).kurt()\n",
        "    df['trailing_2.75h_skew_pow'] = df[power_col].rolling(window=11, min_periods=1).skew()\n",
        "    df['trailing_2.75h_kurt_pow'] = df[power_col].rolling(window=11, min_periods=1).kurt()\n",
        "\n",
        "    df['trailing_3h_skew_pow'] = df[power_col].rolling(window=12, min_periods=1).skew()\n",
        "    df['trailing_3h_kurt_pow'] = df[power_col].rolling(window=12, min_periods=1).kurt()\n",
        "    df['trailing_3.25h_skew_pow'] = df[power_col].rolling(window=13, min_periods=1).skew()\n",
        "    df['trailing_3.25h_kurt_pow'] = df[power_col].rolling(window=13, min_periods=1).kurt()\n",
        "    df['trailing_3.5h_skew_pow'] = df[power_col].rolling(window=14, min_periods=1).skew()\n",
        "    df['trailing_3.5h_kurt_pow'] = df[power_col].rolling(window=14, min_periods=1).kurt()\n",
        "    df['trailing_3.75h_skew_pow'] = df[power_col].rolling(window=15, min_periods=1).skew()\n",
        "    df['trailing_3.75h_kurt_pow'] = df[power_col].rolling(window=15, min_periods=1).kurt()\n",
        "\n",
        "    df['trailing_4h_skew_pow'] = df[power_col].rolling(window=16, min_periods=1).skew()\n",
        "    df['trailing_4h_kurt_pow'] = df[power_col].rolling(window=16, min_periods=1).kurt()\n",
        "    df['trailing_5h_skew_pow'] = df[power_col].rolling(window=24, min_periods=1).skew()\n",
        "    df['trailing_5h_kurt_pow'] = df[power_col].rolling(window=24, min_periods=1).kurt()\n",
        "    df['trailing_6h_skew_pow'] = df[power_col].rolling(window=24, min_periods=1).skew()\n",
        "    df['trailing_6h_kurt_pow'] = df[power_col].rolling(window=24, min_periods=1).kurt()\n",
        "    df['trailing_7h_skew_pow'] = df[power_col].rolling(window=28, min_periods=1).skew()\n",
        "    df['trailing_7h_kurt_pow'] = df[power_col].rolling(window=28, min_periods=1).kurt()\n",
        "    df['trailing_8h_skew_pow'] = df[power_col].rolling(window=32, min_periods=1).skew()\n",
        "    df['trailing_8h_kurt_pow'] = df[power_col].rolling(window=32, min_periods=1).kurt()\n",
        "    df['trailing_9h_skew_pow'] = df[power_col].rolling(window=36, min_periods=1).skew()\n",
        "    df['trailing_9h_kurt_pow'] = df[power_col].rolling(window=36, min_periods=1).kurt()\n",
        "    df['trailing_10h_skew_pow'] = df[power_col].rolling(window=40, min_periods=1).skew()\n",
        "    df['trailing_10h_kurt_pow'] = df[power_col].rolling(window=40, min_periods=1).kurt()\n",
        "    df['trailing_11h_skew_pow'] = df[power_col].rolling(window=44, min_periods=1).skew()\n",
        "    df['trailing_11h_kurt_pow'] = df[power_col].rolling(window=44, min_periods=1).kurt()\n",
        "    df['trailing_12h_skew_pow'] = df[power_col].rolling(window=48, min_periods=1).skew()\n",
        "    df['trailing_12h_kurt_pow'] = df[power_col].rolling(window=48, min_periods=1).kurt()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # ----------------------\n",
        "    # Temperature features (mirrors power features)\n",
        "    # ----------------------\n",
        "    temp_col = 'Dry_Bulb_Temperature_C'\n",
        "\n",
        "    # trailing deltas for temperature (same shift pattern as power)\n",
        "    df['trailing_15m_temp_delta'] = df[temp_col] - df[temp_col].shift(1)\n",
        "    df['trailing_30m_temp_delta'] = df[temp_col] - df[temp_col].shift(2)\n",
        "    df['trailing_45m_temp_delta'] = df[temp_col] - df[temp_col].shift(3)\n",
        "    df['trailing_60m_temp_delta'] = df[temp_col] - df[temp_col].shift(4)\n",
        "    df['trailing_1.25_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(5)\n",
        "    df['trailing_1.5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(6)\n",
        "    df['trailing_1.75_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(7)\n",
        "    df['trailing_2_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(8)\n",
        "    df['trailing_2.25_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(9)\n",
        "    df['trailing_2.5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(10)\n",
        "    df['trailing_2.75_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(11)\n",
        "    df['trailing_3_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(12)\n",
        "    df['trailing_3.25_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(13)\n",
        "    df['trailing_3.5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(14)\n",
        "    df['trailing_3.75_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(15)\n",
        "    df['trailing_4_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(16)\n",
        "    df['trailing_4.25_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(17)\n",
        "    df['trailing_4.5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(18)\n",
        "    df['trailing_4.75_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(19)\n",
        "    df['trailing_5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(20)\n",
        "    df['trailing_5.25_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(21)\n",
        "    df['trailing_5.5_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(22)\n",
        "    df['trailing_5.75_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(23)\n",
        "    df['trailing_6_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(24)\n",
        "    df['trailing_7_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(28)\n",
        "    df['trailing_8_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(32)\n",
        "    df['trailing_9_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(36)\n",
        "    df['trailing_10_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(40)\n",
        "    df['trailing_11_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(44)\n",
        "    df['trailing_12_hours_temp_delta'] = df[temp_col] - df[temp_col].shift(48)\n",
        "\n",
        "    # rolling aggregates for temperature\n",
        "    for window, label in [(2,'0.5'),(4, '1h'),(6,'1.5h'), (8, '2h'),(10,'2.5h'), (16, '4h'),(48,'12h')]:\n",
        "        df[f'rolling_{label}_avg_temp'] = df[temp_col].rolling(window, min_periods=1).mean()\n",
        "        df[f'rolling_{label}_min_temp'] = df[temp_col].rolling(window, min_periods=1).min()\n",
        "        df[f'rolling_{label}_max_temp'] = df[temp_col].rolling(window, min_periods=1).max()\n",
        "\n",
        "    # rolling skew/kurt for temperature\n",
        "    df['trailing_2h_skew_temp'] = df[temp_col].rolling(window=8, min_periods=1).skew()\n",
        "    df['trailing_2h_kurt_temp'] = df[temp_col].rolling(window=8, min_periods=1).kurt()\n",
        "    df['trailing_2.25h_skew_temp'] = df[temp_col].rolling(window=9, min_periods=1).skew()\n",
        "    df['trailing_2.25h_kurt_temp'] = df[temp_col].rolling(window=9, min_periods=1).kurt()\n",
        "    df['trailing_2.5h_skew_temp'] = df[temp_col].rolling(window=10, min_periods=1).skew()\n",
        "    df['trailing_2.5h_kurt_temp'] = df[temp_col].rolling(window=10, min_periods=1).kurt()\n",
        "    df['trailing_2.75h_skew_temp'] = df[temp_col].rolling(window=11, min_periods=1).skew()\n",
        "    df['trailing_2.75h_kurt_temp'] = df[temp_col].rolling(window=11, min_periods=1).kurt()\n",
        "\n",
        "    df['trailing_3h_skew_temp'] = df[temp_col].rolling(window=12, min_periods=1).skew()\n",
        "    df['trailing_3h_kurt_temp'] = df[temp_col].rolling(window=12, min_periods=1).kurt()\n",
        "    df['trailing_3.25h_skew_temp'] = df[temp_col].rolling(window=13, min_periods=1).skew()\n",
        "    df['trailing_3.25h_kurt_temp'] = df[temp_col].rolling(window=13, min_periods=1).kurt()\n",
        "    df['trailing_3.5h_skew_temp'] = df[temp_col].rolling(window=14, min_periods=1).skew()\n",
        "    df['trailing_3.5h_kurt_temp'] = df[temp_col].rolling(window=14, min_periods=1).kurt()\n",
        "    df['trailing_3.75h_skew_temp'] = df[temp_col].rolling(window=15, min_periods=1).skew()\n",
        "    df['trailing_3.75h_kurt_temp'] = df[temp_col].rolling(window=15, min_periods=1).kurt()\n",
        "\n",
        "    df['trailing_4h_skew_temp'] = df[temp_col].rolling(window=16, min_periods=1).skew()\n",
        "    df['trailing_4h_kurt_temp'] = df[temp_col].rolling(window=16, min_periods=1).kurt()\n",
        "    df['trailing_5h_skew_temp'] = df[temp_col].rolling(window=24, min_periods=1).skew()\n",
        "    df['trailing_5h_kurt_temp'] = df[temp_col].rolling(window=24, min_periods=1).kurt()\n",
        "    df['trailing_6h_skew_temp'] = df[temp_col].rolling(window=24, min_periods=1).skew()\n",
        "    df['trailing_6h_kurt_temp'] = df[temp_col].rolling(window=24, min_periods=1).kurt()\n",
        "    df['trailing_7h_skew_temp'] = df[temp_col].rolling(window=28, min_periods=1).skew()\n",
        "    df['trailing_7h_kurt_temp'] = df[temp_col].rolling(window=28, min_periods=1).kurt()\n",
        "    df['trailing_8h_skew_temp'] = df[temp_col].rolling(window=32, min_periods=1).skew()\n",
        "    df['trailing_8h_kurt_temp'] = df[temp_col].rolling(window=32, min_periods=1).kurt()\n",
        "    df['trailing_9h_skew_temp'] = df[temp_col].rolling(window=36, min_periods=1).skew()\n",
        "    df['trailing_9h_kurt_temp'] = df[temp_col].rolling(window=36, min_periods=1).kurt()\n",
        "    df['trailing_10h_skew_temp'] = df[temp_col].rolling(window=40, min_periods=1).skew()\n",
        "    df['trailing_10h_kurt_temp'] = df[temp_col].rolling(window=40, min_periods=1).kurt()\n",
        "    df['trailing_11h_skew_temp'] = df[temp_col].rolling(window=44, min_periods=1).skew()\n",
        "    df['trailing_11h_kurt_temp'] = df[temp_col].rolling(window=44, min_periods=1).kurt()\n",
        "    df['trailing_12h_skew_temp'] = df[temp_col].rolling(window=48, min_periods=1).skew()\n",
        "    df['trailing_12h_kurt_temp'] = df[temp_col].rolling(window=48, min_periods=1).kurt()\n",
        "\n",
        "\n",
        "    # ----------------------\n",
        "    # 1) Build daily-series: one row per (date, time_of_day)\n",
        "    # aggregate BOTH power and temp to ensure one value per (date,time)\n",
        "    # ----------------------\n",
        "    agg_cols = [power_col, temp_col]\n",
        "    daily = (\n",
        "        df.groupby(['date', 'time_of_day'])[agg_cols]\n",
        "          .mean()            # if multiple rows for same (date,time) -> average them\n",
        "          .reset_index()\n",
        "          .sort_values(['time_of_day', 'date'])\n",
        "    )\n",
        "\n",
        "    # convenience short names for feature naming\n",
        "    col_map = {power_col: 'power', temp_col: 'temp'}\n",
        "\n",
        "    # ----------------------\n",
        "    # 2) For each time_of_day group, compute rolling (on days) over shifted series (exclude current)\n",
        "    # The loop below computes previous-day aggregates for multiple lookback windows\n",
        "    # ----------------------\n",
        "    for w in windows:\n",
        "        prefix = f'prev{w}d'   # e.g. prev3d_mean_power, prev7d_std_temp...\n",
        "        # handle each column separately\n",
        "\n",
        "        for col in agg_cols:\n",
        "            short = col_map[col]\n",
        "            grp = daily.groupby('time_of_day')[col]\n",
        "\n",
        "            # special-case w==1 to only compute mean using a shifted window\n",
        "            if w==1:\n",
        "              daily[f'{prefix}_mean_{short}'] = grp.transform(\n",
        "                  lambda s: s.shift(1).rolling(window=w, min_periods=min_periods).mean()\n",
        "              )\n",
        "            else:\n",
        "              # lag value (exact value w days ago at same time_of_day)\n",
        "              daily[f'lag_{w}d_{short}'] = grp.transform(lambda s: s.shift(w))\n",
        "\n",
        "              # rolling aggregates over the previous w days (exclude current day via shift(1))\n",
        "              daily[f'{prefix}_mean_{short}'] = grp.transform(\n",
        "                  lambda s: s.shift(1).rolling(window=w, min_periods=min_periods).mean()\n",
        "              )\n",
        "\n",
        "              daily[f'{prefix}_min_{short}'] = grp.transform(\n",
        "                  lambda s: s.shift(1).rolling(window=w, min_periods=min_periods).min()\n",
        "              )\n",
        "              daily[f'{prefix}_max_{short}'] = grp.transform(\n",
        "                  lambda s: s.shift(1).rolling(window=w, min_periods=min_periods).max()\n",
        "              )\n",
        "\n",
        "\n",
        "\n",
        "    # ----------------------\n",
        "    # 3) Merge daily-level features back to original timestamps by (date, time_of_day)\n",
        "    # This aligns each original row with statistics computed from prior days at the same clock time\n",
        "    # ----------------------\n",
        "    daily_feats = daily.drop(columns=[power_col, temp_col])\n",
        "    df = df.merge(daily_feats, on=['date', 'time_of_day'], how='left', validate='m:1')\n",
        "    # cleanup helper columns\n",
        "    df.drop(columns=['date', 'time_of_day'], inplace=True)\n",
        "\n",
        "    return df, daily\n",
        "\n",
        "\n",
        "# example usage (original calls kept unchanged):\n",
        "train_df, _ = create_timeofday_pastdays_features(train_df)\n",
        "test_df, _ = create_timeofday_pastdays_features(test_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c665c661",
      "metadata": {
        "collapsed": true,
        "id": "c665c661"
      },
      "outputs": [],
      "source": [
        "def preprocess_nan_values(df):\n",
        "    # Replace infinite values with NaN to avoid issues in calculations\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Fill NaN values with 0 (could also use median or other strategies depending on data)\n",
        "    df.fillna(0, inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply preprocessing to training data\n",
        "train_df = preprocess_nan_values(train_df)\n",
        "# Apply preprocessing to test data\n",
        "test_df = preprocess_nan_values(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CUAUpJuTz5-K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUAUpJuTz5-K",
        "outputId": "e6dba27f-2e0d-45b2-895a-de225633a49f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(35040, 314)\n",
            "(105120, 313)\n"
          ]
        }
      ],
      "source": [
        "# Print the number of rows and columns in the training and test datasets\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ToZI_RsqbyMc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ToZI_RsqbyMc",
        "outputId": "48799daf-d25d-49d1-9d07-019d330f654a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['Dry_Bulb_Temperature_C', 'Global_Horizontal_Radiation_W/m2', 'Building_Power_kW', 'Demand_Response_Flag', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'weekday_sin', 'weekday_cos',\n",
            "       ...\n",
            "       'prev13d_max_temp', 'lag_14d_power', 'prev14d_mean_power', 'prev14d_min_power', 'prev14d_max_power', 'lag_14d_temp', 'prev14d_mean_temp', 'prev14d_min_temp', 'prev14d_max_temp', 'Demand_Response_Capacity_kW'], dtype='object', length=312)\n"
          ]
        }
      ],
      "source": [
        "# Select only numeric columns from train and test data\n",
        "num_train = train_df.select_dtypes(include=[np.number]).copy()\n",
        "num_test = test_df.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "# Extract target values for training set and reshape to 2D array\n",
        "y_train_raw = num_train[TARGET_COL].values.reshape(-1, 1)\n",
        "# Features for training (all numeric columns except target)\n",
        "X_train_num = num_train.drop(columns=[TARGET_COL])\n",
        "\n",
        "# Ensure all columns in training data exist in test data, fill missing columns with 0\n",
        "for col in num_train.columns:\n",
        "    if col not in num_test.columns:\n",
        "        num_test[col] = 0.0\n",
        "\n",
        "# Print test numeric columns to verify alignment\n",
        "print(num_test.columns)\n",
        "\n",
        "# Extract target values for test set and reshape to 2D array\n",
        "y_test_raw = num_test[TARGET_COL].values.reshape(-1, 1)\n",
        "# Features for test set (all numeric columns except target)\n",
        "X_test_num = num_test.drop(columns=[TARGET_COL])\n",
        "\n",
        "# Align the order of test features to match training features\n",
        "X_test_num = X_test_num[X_train_num.columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21eb0121",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21eb0121",
        "outputId": "3eca26f9-5e48-41ea-8a39-bc6d6dad54a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Scaling done. X_train_scaled: (35040, 311) y_train_scaled: (35040, 1)\n"
          ]
        }
      ],
      "source": [
        "# Initialize MinMaxScaler to scale features and target between -1 and 1\n",
        "X_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "y_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "\n",
        "# Fit the scaler on training features and transform both train and test sets\n",
        "X_train_scaled = X_scaler.fit_transform(X_train_num.values)\n",
        "X_test_scaled = X_scaler.transform(X_test_num.values)\n",
        "\n",
        "# Convert scaled feature arrays to float32 for efficient computation\n",
        "X_train_scaled = X_train_scaled.astype('float32')\n",
        "X_test_scaled = X_test_scaled.astype('float32')\n",
        "\n",
        "# Fit the scaler on training target and transform both train and test targets\n",
        "y_train_scaled = y_scaler.fit_transform(y_train_raw)\n",
        "y_test_scaled = y_scaler.transform(y_test_raw)\n",
        "\n",
        "# Print confirmation and shapes of scaled arrays\n",
        "print('[INFO] Scaling done. X_train_scaled:', X_train_scaled.shape, 'y_train_scaled:', y_train_scaled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63226a6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63226a6b",
        "outputId": "db778f3f-04c4-4c63-cec2-6b88f22c90de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Train sequences: (34849, 192, 311) (34849, 1)\n",
            "[INFO] Test sequences: (104547, 192, 311)\n"
          ]
        }
      ],
      "source": [
        "WINDOW_SIZE = 96 #our train df includes timestamps from 7:00 to 19:00 so it makes 48 rows and we thought it would be good if short memory stores the last 2 days information (we also tried different window sizes but this was the most efficient one)\n",
        "\n",
        "# Function to create rolling sequences for each site (or globally if no site column)\n",
        "def make_sequences_by_site(df_raw, X_scaled_all, y_scaled_all, window_size, site_col=SITE_COL, time_col=TIME_COL, have_target=True):\n",
        "    # Initialize lists to hold sequences, corresponding targets, and end indices\n",
        "    X_seq_list, y_seq_list, end_indices = [], [], []\n",
        "    n_features = X_scaled_all.shape[1]\n",
        "\n",
        "    # Check if the dataset contains a site column\n",
        "    if site_col in df_raw.columns:\n",
        "        # Process each site separately\n",
        "        for site, g in df_raw.groupby(site_col):\n",
        "            idx = g.index.values\n",
        "            Xg = X_scaled_all[idx]  # select corresponding feature rows\n",
        "            yg = y_scaled_all[idx] if (have_target and y_scaled_all is not None) else None\n",
        "\n",
        "            # Create sequences of length `window_size`\n",
        "            for i in range(len(idx) - window_size + 1):\n",
        "                X_seq_list.append(Xg[i:i+window_size])  # features sequence\n",
        "                end_indices.append(idx[i+window_size-1])  # index of last element in sequence\n",
        "                if have_target and yg is not None:\n",
        "                    y_seq_list.append(yg[i+window_size-1])  # target corresponding to last element\n",
        "    else:\n",
        "        # If no site column, process the entire dataset as a single group\n",
        "        idx = np.arange(len(df_raw))\n",
        "        Xg = X_scaled_all\n",
        "        yg = y_scaled_all if (have_target and y_scaled_all is not None) else None\n",
        "        for i in range(len(idx) - window_size + 1):\n",
        "            X_seq_list.append(Xg[i:i+window_size])\n",
        "            end_indices.append(i+window_size-1)\n",
        "            if have_target and yg is not None:\n",
        "                y_seq_list.append(yg[i+window_size-1])\n",
        "\n",
        "    # Convert lists to numpy arrays\n",
        "    X_seq = np.array(X_seq_list)\n",
        "    y_seq = np.array(y_seq_list) if have_target and y_seq_list else None\n",
        "\n",
        "    return X_seq, y_seq, np.array(end_indices)\n",
        "\n",
        "# Generate training sequences (with target)\n",
        "X_train_seq, y_train_seq, train_end_idx = make_sequences_by_site(train_df, X_train_scaled, y_train_scaled, WINDOW_SIZE, have_target=True)\n",
        "print('[INFO] Train sequences:', X_train_seq.shape, y_train_seq.shape)\n",
        "\n",
        "# Generate test sequences (without target)\n",
        "X_test_seq, _, test_end_idx = make_sequences_by_site(test_df, X_test_scaled, None, WINDOW_SIZE, have_target=False)\n",
        "print('[INFO] Test sequences:', X_test_seq.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XFP9Ngx2S2OS",
      "metadata": {
        "id": "XFP9Ngx2S2OS"
      },
      "outputs": [],
      "source": [
        "# Number of features in the input sequence\n",
        "n_features = X_train_seq.shape[-1]\n",
        "\n",
        "# Define the input layer for the model\n",
        "# The input shape is (WINDOW_SIZE, n_features)\n",
        "# WINDOW_SIZE is the number of timesteps in the sequence\n",
        "# n_features is the number of features at each timestep\n",
        "inputs = keras.Input(shape=(WINDOW_SIZE, n_features))\n",
        "\n",
        "# First LSTM layer with 128 units\n",
        "# return_sequences=True is used so that the next LSTM layer receives a full sequence\n",
        "# This helps the network capture temporal dependencies across all timesteps\n",
        "x = layers.LSTM(128, return_sequences=True)(inputs)\n",
        "\n",
        "# Second LSTM layer with 64 units\n",
        "# return_sequences=False because this is the last LSTM layer\n",
        "# It outputs a single vector representing the entire sequence\n",
        "x = layers.LSTM(64, return_sequences=False)(x)\n",
        "\n",
        "# Dropout layer with rate 0.3\n",
        "# Helps prevent overfitting by randomly setting 30% of inputs to zero during training\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "# Dense layer with 64 units and ReLU activation\n",
        "# Adds non-linear transformation and allows the model to learn complex representations\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "# Dropout layer with rate 0.2\n",
        "# Additional regularization to further reduce overfitting\n",
        "x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# Output layer with 1 unit\n",
        "# Suitable for regression tasks (predicting a single continuous value)\n",
        "outputs = layers.Dense(1)(x)\n",
        "\n",
        "# Create the model by specifying the input and output layers\n",
        "model = keras.Model(inputs, outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dj8r2m1Q3v_M",
      "metadata": {
        "collapsed": true,
        "id": "Dj8r2m1Q3v_M"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer\n",
        "# Adam is a popular choice for sequence models because it adapts the learning rate for each parameter\n",
        "# learning_rate=5e-4 is relatively small, which helps with stable training\n",
        "optimizer = Adam(learning_rate=5e-4)\n",
        "\n",
        "# Compile the model\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss='mse',\n",
        "    metrics=['mae', tf.keras.metrics.RootMeanSquaredError()]\n",
        ")\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "# Shows each layer, output shapes, number of parameters, and total trainable parameters\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199b6774",
      "metadata": {
        "collapsed": true,
        "id": "199b6774"
      },
      "outputs": [],
      "source": [
        "# Define callbacks for training\n",
        "callbacks = [\n",
        "    # EarlyStopping: stops training if validation loss doesn't improve for 'patience' epochs\n",
        "    # restore_best_weights=True ensures the model returns to the weights with the lowest validation loss\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "\n",
        "    # ReduceLROnPlateau: reduces learning rate if validation loss stops improving\n",
        "    # This helps the model converge better if it hits a plateau\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        patience=10\n",
        "    ),\n",
        "\n",
        "    # ModelCheckpoint: saves the model with the best validation loss during training\n",
        "    # This ensures you don't lose the best version of your model\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='best_lstm_model.keras',\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True\n",
        "    )\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_seq,            # Input training sequences\n",
        "    y_train_seq,            # Target values for each sequence\n",
        "    epochs=50,              # Maximum number of epochs to train\n",
        "    batch_size=128,         # Number of samples per gradient update\n",
        "    validation_split=0.2,   # Use 20% of training data as validation set\n",
        "    callbacks=callbacks,     # Apply the callbacks defined above\n",
        "    verbose=1                # Print training progress per epoch\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37d4928a",
      "metadata": {
        "id": "37d4928a"
      },
      "outputs": [],
      "source": [
        "# Recreate the model architecture and load the best weights\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.load_weights('/content/best_lstm_model.keras')  # Load saved weights from training\n",
        "\n",
        "print('[LOADED] Model loaded successfully using weights.')\n",
        "print('[LOADED] Scalers loaded successfully.')\n",
        "\n",
        "# Ensure that the timestamp column is in datetime format\n",
        "if TIME_COL in test_df.columns:\n",
        "    test_df[TIME_COL] = pd.to_datetime(test_df[TIME_COL])\n",
        "\n",
        "# Make predictions on the test sequences\n",
        "# The output will be scaled values because the model was trained on scaled data\n",
        "y_test_pred_scaled = model.predict(X_test_seq, verbose=0)\n",
        "\n",
        "# Convert scaled predictions back to original scale using the inverse transform of the y_scaler\n",
        "y_test_pred = y_scaler.inverse_transform(y_test_pred_scaled)\n",
        "\n",
        "# Initialize an array to hold predicted capacities for the full test DataFrame\n",
        "pred_capacity = np.full((len(test_df),), np.nan, dtype=float)\n",
        "\n",
        "# Retrieve flags if available, otherwise assume zeros\n",
        "flags = test_df[FLAG_COL].fillna(0).values if FLAG_COL in test_df.columns else np.zeros(len(test_df))\n",
        "\n",
        "# Map the model predictions to the correct indices in the original test DataFrame\n",
        "for i, end_idx in enumerate(test_end_idx):\n",
        "    pred_capacity[end_idx] = y_test_pred[i]\n",
        "\n",
        "# Prepare signed predictions\n",
        "# Only keep predictions where the flag is non-zero and a valid prediction exists\n",
        "signed_pred = np.zeros_like(pred_capacity)\n",
        "non_zero_mask = (flags != 0) & ~np.isnan(pred_capacity)\n",
        "signed_pred[non_zero_mask] = pred_capacity[non_zero_mask]\n",
        "\n",
        "# Replace remaining NaNs with 0\n",
        "signed_pred = np.nan_to_num(signed_pred, nan=0.0)\n",
        "\n",
        "# Create the output DataFrame\n",
        "out_df = pd.DataFrame({\n",
        "    SITE_COL: test_df[SITE_COL].values if SITE_COL in test_df.columns else 'NA',\n",
        "    TIME_COL: pd.to_datetime(test_df[TIME_COL]).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "              if TIME_COL in test_df.columns else np.arange(len(test_df)),\n",
        "    FLAG_COL: flags,\n",
        "    'Demand_Response_Capacity_kW': signed_pred\n",
        "})\n",
        "\n",
        "# Sort the output by site and timestamp and reset index\n",
        "out_df = out_df.sort_values([SITE_COL, TIME_COL]).reset_index(drop=True)\n",
        "\n",
        "# Save predictions to CSV\n",
        "out_df.to_csv(PREDICTIONS_CSV, index=False)\n",
        "print('[OK] Predictions saved to:', PREDICTIONS_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hZ1NlN6jr837",
      "metadata": {
        "id": "hZ1NlN6jr837"
      },
      "outputs": [],
      "source": [
        "# Define file paths for saving model weights and scalers\n",
        "weights_path = os.path.join('LSTM_capacity_weights.weights.h5')  # Model weights will be saved here\n",
        "features_scaler_path = os.path.join('features_scaler.joblib')    # Scaler for input features\n",
        "target_scaler_path = os.path.join('target_scaler.joblib')        # Scaler for target/output values\n",
        "\n",
        "# Save only the model weights\n",
        "model.save_weights(weights_path)\n",
        "\n",
        "# Save the scalers used for preprocessing\n",
        "joblib.dump(X_scaler, features_scaler_path)  # Save the input feature scaler\n",
        "joblib.dump(y_scaler, target_scaler_path)    # Save the target/output scaler\n",
        "\n",
        "# Confirmation messages to inform the user\n",
        "print('[SAVED] Model weights:', weights_path)\n",
        "print('[SAVED] Feature scaler:', features_scaler_path)\n",
        "print('[SAVED] Target scaler:', target_scaler_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}