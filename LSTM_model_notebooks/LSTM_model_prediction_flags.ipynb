{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "448b1622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.12.0\n",
      "NumPy version: 1.23.5\n",
      "Pandas version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Input, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378a51da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Explore Data\n",
    "df_b = pd.read_csv(\"\") # train\n",
    "df_d = pd.read_csv(\"\") # validation\"\"\"\n",
    "df_c = pd.read_csv(\"\") # test\n",
    "\n",
    "df_b = df_b[df_b['Timestamp_Local'].str.slice(11, 16).between('07:00', '19:00')]\n",
    "df_d = df_d[df_d['Timestamp_Local'].str.slice(11, 16).between('07:00', '19:00')]\n",
    "df_c = df_c[df_c['Timestamp_Local'].str.slice(11, 16).between('07:00', '19:00')]\n",
    "\n",
    "print(\"Site B shape:\", df_b.shape)\n",
    "print(\"Site D shape:\", df_d.shape)\n",
    "print(\"Site C shape:\", df_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_hourly_averages(df):\n",
    "    \"\"\"Calculate actual hourly average power from training data\"\"\"\n",
    "    df_temp = df.copy()\n",
    "    df_temp['Timestamp_Local'] = pd.to_datetime(df_temp['Timestamp_Local'])\n",
    "    df_temp['hour'] = df_temp['Timestamp_Local'].dt.hour\n",
    "    \n",
    "    # Calculate hourly averages for business hours (10-18)\n",
    "    business_hours = range(10, 19)  # 10 to 18 inclusive\n",
    "    hourly_averages = {}\n",
    "    \n",
    "    print(\"Calculating hourly averages from Site B training data:\")\n",
    "    for hour in business_hours:\n",
    "        hour_data = df_temp[df_temp['hour'] == hour]['Building_Power_kW']\n",
    "        if len(hour_data) > 0:\n",
    "            avg_power = hour_data.mean()\n",
    "            hourly_averages[hour] = avg_power\n",
    "            print(f\"Hour {hour}: {avg_power:.2f} kW (from {len(hour_data)} samples)\")\n",
    "        else:\n",
    "            print(f\"Hour {hour}: No data available\")\n",
    "    \n",
    "    return hourly_averages\n",
    "\n",
    "\n",
    "def create_enhanced_features(df, hourly_avg_power=None):\n",
    "    \"\"\"\n",
    "    Create time-based features and power deviation features from timestamp and building power columns\n",
    "    Business hours considered 10:00-18:00 inclusive.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    df['Timestamp_Local'] = pd.to_datetime(df['Timestamp_Local'])\n",
    "    \n",
    "    # Extract hour and month\n",
    "    df['hour'] = df['Timestamp_Local'].dt.hour\n",
    "    df['month'] = df['Timestamp_Local'].dt.month\n",
    "    \n",
    "    # === TIME-BASED FEATURES ===\n",
    "    # Create is_10_to_18 feature (business hours: 10:00 AM - 6:00 PM inclusive)\n",
    "    df['is_10_to_18'] = ((df['hour'] >= 10) & (df['hour'] <= 17)).astype(int)\n",
    "    \n",
    "    df['is_10'] = (df['hour'] == 10).astype(int)\n",
    "    df['is_11'] = (df['hour'] == 11).astype(int)\n",
    "    df['is_12'] = (df['hour'] == 12).astype(int)\n",
    "    df['is_13'] = (df['hour'] == 13).astype(int)\n",
    "    df['is_14'] = (df['hour'] == 14).astype(int)\n",
    "    df['is_15'] = (df['hour'] == 15).astype(int)\n",
    "    df['is_16'] = (df['hour'] == 16).astype(int)\n",
    "    df['is_17'] = (df['hour'] == 17).astype(int)\n",
    "\n",
    "    # Create monthly indicator features\n",
    "    df['is_january'] = (df['month'] == 1).astype(int)\n",
    "    df['is_february'] = (df['month'] == 2).astype(int)\n",
    "    df['is_june'] = (df['month'] == 6).astype(int)\n",
    "    df['is_july'] = (df['month'] == 7).astype(int)\n",
    "    df['is_august'] = (df['month'] == 8).astype(int)\n",
    "    df['is_december'] = (df['month'] == 12).astype(int)\n",
    "    \n",
    "    # === PATTERN + DEVIATION FEATURES ===\n",
    "    # Hour-specific pattern flags (updated bounds)\n",
    "    df['is_morning_peak'] = ((df['hour'] >= 10) & (df['hour'] <= 11)).astype(int)  # 10-12\n",
    "    df['is_afternoon_low'] = ((df['hour'] >= 12) & (df['hour'] <= 17)).astype(int) # 13-18\n",
    "    \n",
    "    # Use calculated hourly averages if provided, otherwise fallback to overall mean\n",
    "    if hourly_avg_power is None:\n",
    "        print(\"Warning: Using overall mean as no hourly averages provided\")\n",
    "        df['expected_power'] = df['Building_Power_kW'].mean()\n",
    "    else:\n",
    "        # Power deviation from calculated hourly average\n",
    "        df['expected_power'] = df['hour'].map(hourly_avg_power).fillna(df['Building_Power_kW'].mean())\n",
    "    \n",
    "    df['power_deviation'] = df['Building_Power_kW'] - df['expected_power']\n",
    "    df['power_deviation_ratio'] = df['power_deviation'] / df['expected_power']\n",
    "    \n",
    "    # Drop the temporary expected_power column as we have the deviation features\n",
    "    df = df.drop(columns=['expected_power'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_learnable_signals(df, hourly_avg_power=None, seq_length=192):\n",
    "    \"\"\"\n",
    "    Create learnable signal features for LSTM sequences (computed on-the-fly, not persisted).\n",
    "    These are goal-aligned features that help the model learn the domain rules.\n",
    "    Returns: df with additional computed columns for use in sequences\n",
    "    \"\"\"    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # === CYCLICAL TIME ENCODING ===\n",
    "    # Hour cyclical (0-23 -> 0-2π)\n",
    "    hour_rad = 2 * np.pi * df['hour'] / 24\n",
    "    df['hour_sin'] = np.sin(hour_rad)\n",
    "    df['hour_cos'] = np.cos(hour_rad)\n",
    "    \n",
    "    # Month cyclical (1-12 -> 0-2π)\n",
    "    month_rad = 2 * np.pi * (df['month'] - 1) / 12\n",
    "    df['month_sin'] = np.sin(month_rad)\n",
    "    df['month_cos'] = np.cos(month_rad)\n",
    "    \n",
    "    # === SOFT MASKS ===\n",
    "    # Business hours soft mask (already have is_10_to_18, but rename for clarity)\n",
    "    df['is_in_business_hours'] = df['is_10_to_18']\n",
    "    \n",
    "    # Position within business hours window (0-1 scale)\n",
    "    df['position_in_window'] = np.where(\n",
    "        df['is_in_business_hours'] == 1,\n",
    "        (df['hour'] - 10) / 8,  # 10->0, 18->1\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    # Allowed month soft mask\n",
    "    allowed_months = {1, 2, 6, 7, 8, 12}\n",
    "    df['is_allowed_month'] = df['month'].isin(allowed_months).astype(int)\n",
    "    \n",
    "    # === CAUSAL POWER DELTAS ===\n",
    "    power_col = 'Building_Power_kW'\n",
    "    \n",
    "    # Rolling deltas (causal - looking back only)\n",
    "    # 15-minute delta (assuming 15-min intervals)\n",
    "    df['trailing_15m_delta'] = df[power_col] - df[power_col].shift(1)\n",
    "    \n",
    "    # 30-minute delta (2 periods back)\n",
    "    df['trailing_30m_delta'] = df[power_col] - df[power_col].shift(2)\n",
    "    \n",
    "    # 60-minute average (4 periods back, including current)\n",
    "    df['trailing_60m_avg'] = df[power_col].rolling(window=4, min_periods=1).mean()\n",
    "    \n",
    "    # === RESIDUALS (EXPECTED VS ACTUAL) ===\n",
    "    # Use calculated hourly expectations instead of hardcoded values\n",
    "    if hourly_avg_power is None:\n",
    "        print(\"Warning: Using overall mean for residuals as no hourly averages provided\")\n",
    "        expected_power = df[power_col].mean()\n",
    "    else:\n",
    "        expected_power = df['hour'].map(hourly_avg_power).fillna(df[power_col].mean())\n",
    "    \n",
    "    df['residual'] = df[power_col] - expected_power\n",
    "    df['residual_ratio'] = df['residual'] / np.maximum(expected_power, 0.1)  # avoid div by zero\n",
    "    \n",
    "    # 15-minute slope\n",
    "    df['slope_15m'] = df['trailing_15m_delta'] / 0.25  # delta per hour\n",
    "    \n",
    "    # === MORNING VS EVENING CUES ===\n",
    "    # Add date for daily grouping\n",
    "    df['date'] = df['Timestamp_Local'].dt.date\n",
    "    \n",
    "    # Morning baseline (10:45-11:15 average, carried forward for the day)\n",
    "    # For simplicity, use hour 11 as proxy for morning baseline\n",
    "    morning_mask = (df['hour'] == 11)\n",
    "    daily_morning_baseline = df[morning_mask].groupby('date')[power_col].mean()\n",
    "    \n",
    "    # Map back to all rows for each date\n",
    "    df['morning_baseline'] = df['date'].map(daily_morning_baseline).fillna(0)\n",
    "    df['morning_baseline_available'] = df['date'].isin(daily_morning_baseline.index).astype(int)\n",
    "    \n",
    "    # Evening trailing average (17:30-18:00 proxy - use hour 18)\n",
    "    # For each row, calculate trailing 30-min if in evening window\n",
    "    df['evening_trailing_30'] = np.where(\n",
    "        df['hour'] >= 17,\n",
    "        df[power_col].rolling(window=2, min_periods=1).mean(),\n",
    "        0\n",
    "    )\n",
    "    df['evening_window_active'] = (df['hour'] >= 17).astype(int)\n",
    "    \n",
    "    # Morning vs evening delta\n",
    "    df['evening_delta'] = df['evening_trailing_30'] - df['morning_baseline']\n",
    "    \n",
    "    # Binary cue for delta > 8 (soft evidence)\n",
    "    df['delta_gt_8'] = (df['evening_delta'] > 8).astype(int)\n",
    "    \n",
    "    # Fill NaN values from rolling operations\n",
    "    numeric_cols = ['trailing_15m_delta', 'trailing_30m_delta', 'trailing_60m_avg', \n",
    "                   'slope_15m', 'evening_delta']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # Drop temporary date column\n",
    "    df = df.drop(columns=['date'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fafe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enhanced features to all datasets\n",
    "print(\"Creating enhanced time-based features for all sites...\")\n",
    "\n",
    "# Calculate hourly averages from Site B training data\n",
    "hourly_averages = calculate_hourly_averages(df_b)\n",
    "\n",
    "# Apply enhanced features with calculated hourly averages\n",
    "df_b_enhanced = create_enhanced_features(df_b, hourly_averages)\n",
    "df_d_enhanced = create_enhanced_features(df_d, hourly_averages)\n",
    "df_c_enhanced = create_enhanced_features(df_c, hourly_averages)\n",
    "\n",
    "print(\"Adding learnable signal features...\")\n",
    "df_b_signals = create_learnable_signals(df_b_enhanced, hourly_averages)\n",
    "df_d_signals = create_learnable_signals(df_d_enhanced, hourly_averages)\n",
    "df_c_signals = create_learnable_signals(df_c_enhanced, hourly_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfbc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Feature Scaling\n",
    "# Remove unnecessary columns but KEEP hour and month for better temporal learning\n",
    "columns_to_drop = [\"Site\", \"Timestamp_Local\"]\n",
    "\n",
    "# For site B, also drop Demand_Response_Capacity_kW if it exists\n",
    "if \"Demand_Response_Capacity_kW\" in df_b_signals.columns:\n",
    "    columns_to_drop.append(\"Demand_Response_Capacity_kW\")\n",
    "\n",
    "df_b_clean = df_b_signals.drop(columns=[col for col in columns_to_drop if col in df_b_signals.columns])\n",
    "df_d_clean = df_d_signals.drop(columns=[col for col in columns_to_drop if col in df_d_signals.columns])\n",
    "df_c_clean = df_c_signals.drop(columns=[col for col in columns_to_drop if col in df_c_signals.columns])\n",
    "\n",
    "print(\"Cleaned dataframe columns (keeping hour and month + all learnable signals):\")\n",
    "print(\"Site B (train):\", len(df_b_clean.columns), \"columns\")\n",
    "print(\"Site D (validation):\", len(df_d_clean.columns), \"columns\") \n",
    "print(\"Site C (test):\", len(df_c_clean.columns), \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_features = ['Dry_Bulb_Temperature_C', 'Global_Horizontal_Radiation_W/m2', 'Building_Power_kW']\n",
    "temporal_features = ['hour', 'month']  # Numerical temporal features\n",
    "time_features = ['is_10_to_18','is_10', 'is_11', 'is_12', 'is_13', 'is_14', 'is_15', 'is_16', 'is_17', 'is_january', 'is_february', 'is_june', 'is_july', 'is_august', 'is_december']\n",
    "\n",
    "power_pattern_features = ['is_morning_peak', 'is_afternoon_low']\n",
    "power_deviation_features = ['power_deviation', 'power_deviation_ratio']\n",
    "\n",
    "cyclical_features = ['hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
    "soft_mask_features = ['is_in_business_hours', 'is_allowed_month', 'position_in_window']\n",
    "causal_delta_features = ['trailing_15m_delta', 'trailing_30m_delta', 'trailing_60m_avg', 'slope_15m']\n",
    "residual_features = ['residual', 'residual_ratio']\n",
    "morning_evening_features = ['morning_baseline', 'evening_trailing_30', 'evening_delta', 'delta_gt_8']\n",
    "availability_features = ['morning_baseline_available', 'evening_window_active']\n",
    "\n",
    "learnable_signal_features = (cyclical_features + soft_mask_features + causal_delta_features + \n",
    "                           residual_features + morning_evening_features + availability_features)\n",
    "\n",
    "all_binary_features = (time_features + power_pattern_features + soft_mask_features + \n",
    "                      morning_evening_features[-1:] + availability_features)  # delta_gt_8 is binary\n",
    "all_numerical_features = (original_features + temporal_features + power_deviation_features + \n",
    "                         cyclical_features + causal_delta_features + residual_features + \n",
    "                         morning_evening_features[:-1] + ['position_in_window'])  # exclude delta_gt_8\n",
    "\n",
    "features = all_numerical_features + all_binary_features\n",
    "\n",
    "target = \"Demand_Response_Flag\"\n",
    "\n",
    "numerical_features_to_scale = all_numerical_features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_b = scaler.fit_transform(df_b_clean[numerical_features_to_scale])  # Fit on training data (Site B)\n",
    "scaled_features_d = scaler.transform(df_d_clean[numerical_features_to_scale])  # Transform validation\n",
    "scaled_features_c = scaler.transform(df_c_clean[numerical_features_to_scale])  # Transform test\n",
    "\n",
    "print(f\"\\nScaled features shape: {scaled_features_b.shape}\")\n",
    "\n",
    "# Function to map scaled numerical features back to dataframes\n",
    "def map_scaled_features(df, numerical_feature_list, scaled_array):\n",
    "    df_copy = df.copy()\n",
    "    for i, col in enumerate(numerical_feature_list):\n",
    "        df_copy[col] = scaled_array[:, i]\n",
    "    return df_copy\n",
    "\n",
    "df_b_final = map_scaled_features(df_b_clean, numerical_features_to_scale, scaled_features_b)\n",
    "df_d_final = map_scaled_features(df_d_clean, numerical_features_to_scale, scaled_features_d)\n",
    "df_c_final = map_scaled_features(df_c_clean, numerical_features_to_scale, scaled_features_c)\n",
    "\n",
    "print(\"\\nFinal preprocessed data shapes:\")\n",
    "print(\"Site B (train):\", df_b_final.shape)\n",
    "print(\"Site D (validation):\", df_d_final.shape)\n",
    "print(\"Site C (test):\", df_c_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa75853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sequences for LSTM\n",
    "def create_sequences(df, feature_cols, target_col, seq_len):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input with enhanced features and learnable signals\n",
    "    \"\"\"\n",
    "    data = df[feature_cols].values\n",
    "    labels = df[target_col].values\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(seq_len, len(df)):\n",
    "        X.append(data[i-seq_len:i])  # past N steps with all features\n",
    "        y.append(labels[i])          # current label\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create sequences with 192 timesteps\n",
    "seq_length = 192\n",
    "print(f\"Creating sequences with length {seq_length}...\")\n",
    "\n",
    "X_b, y_b = create_sequences(df_b_final, features, target, seq_length)\n",
    "X_d, y_d = create_sequences(df_d_final, features, target, seq_length)\n",
    "X_c, y_c = create_sequences(df_c_final, features, target, seq_length)\n",
    "\n",
    "print(\"Sequence shapes:\")\n",
    "print(f\"Site B (train): X={X_b.shape}, y={y_b.shape}\")\n",
    "print(f\"Site D (val):   X={X_d.shape}, y={y_d.shape}\")\n",
    "print(f\"Site C (test):  X={X_c.shape}, y={y_c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d0a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_b.shape[2]\n",
    "seq_len = X_b.shape[1]\n",
    "\n",
    "print(f\"Building LSTM model with {n_features} features and {seq_len} timesteps...\")\n",
    "print(f\"Feature count breakdown: {len(features)} total features\")\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=[seq_len, n_features], return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),  # Additional dropout for regularization\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d72a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original label distribution in y_b:\", np.unique(y_b, return_counts=True))\n",
    "print(\"Original label distribution in y_d:\", np.unique(y_d, return_counts=True))\n",
    "print(\"Original label distribution in y_c:\", np.unique(y_c, return_counts=True))\n",
    "\n",
    "# Convert labels: -1 -> 0, 0 -> 1, 1 -> 2\n",
    "y_b_cat = np.where(y_b == -1, 0, np.where(y_b == 0, 1, 2))  # Training labels\n",
    "y_d_cat = np.where(y_d == -1, 0, np.where(y_d == 0, 1, 2))  # Validation labels\n",
    "# Note: y_c_cat not needed since Site C flags are all 0 and need to be predicted\n",
    "\n",
    "print(\"Converted label distribution:\")\n",
    "print(\"y_b_cat (train):\", np.unique(y_b_cat, return_counts=True))\n",
    "print(\"y_d_cat (validation):\", np.unique(y_d_cat, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e4d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Enhanced LSTM Model\n",
    "print(\"Training enhanced LSTM model...\")\n",
    "print(f\"Training data: {X_b.shape[0]} samples\")\n",
    "print(f\"Validation data: {X_d.shape[0]} samples\")\n",
    "\n",
    "# Add early stopping for better training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_b, y_b_cat,\n",
    "    validation_data=(X_d, y_d_cat),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Site C has {X_c.shape[0]} sequences to predict\")\n",
    "\n",
    "# Make predictions on Site C (no true labels available)\n",
    "y_c_pred_prob = model.predict(X_c, verbose=0)\n",
    "y_c_pred = np.argmax(y_c_pred_prob, axis=1)\n",
    "\n",
    "# Convert categorical predictions back to original label format (-1, 0, 1)\n",
    "y_c_pred_original = np.where(y_c_pred == 0, -1, np.where(y_c_pred == 1, 0, 1))\n",
    "\n",
    "print(f\"Site E prediction shapes: {y_c_pred_original.shape}\")\n",
    "print(\"Site E prediction distribution:\", np.unique(y_c_pred_original, return_counts=True))\n",
    "\n",
    "# Create submission DataFrame in required format\n",
    "start_idx_c = seq_length  # Account for sequence offset\n",
    "submission_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Align length with predictions\n",
    "site_slice = df_c['Site'].iloc[start_idx_c:start_idx_c + len(y_c_pred_original)].copy()\n",
    "# Fill missing/empty values\n",
    "fallback_site = site_slice.mode().iloc[0] if not site_slice.mode().empty else 'siteE'\n",
    "site_slice = site_slice.fillna(fallback_site).replace('', fallback_site)\n",
    "\n",
    "submission_df['Site'] = site_slice\n",
    "submission_df['Timestamp_Local'] = df_c['Timestamp_Local'].iloc[start_idx_c:start_idx_c + len(y_c_pred_original)].values\n",
    "submission_df['Demand_Response_Flag'] = y_c_pred_original\n",
    "submission_df['Demand_Response_Capacity_kW'] = 0\n",
    "\n",
    "\n",
    "print(f\"Submission DataFrame created with shape: {submission_df.shape}\")\n",
    "print(f\"Columns: {list(submission_df.columns)}\")\n",
    "\n",
    "submission_path = ''\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Competition submission file saved to: {submission_path}\")\n",
    "\n",
    "# Show prediction summary\n",
    "print(f\"Total predictions: {len(y_c_pred_original)}\")\n",
    "print(f\"Flag -1 (Negative DR): {np.sum(y_c_pred_original == -1)} ({np.sum(y_c_pred_original == -1)/len(y_c_pred_original)*100:.1f}%)\")\n",
    "print(f\"Flag  0 (No event):    {np.sum(y_c_pred_original == 0)} ({np.sum(y_c_pred_original == 0)/len(y_c_pred_original)*100:.1f}%)\")\n",
    "print(f\"Flag  1 (Positive DR): {np.sum(y_c_pred_original == 1)} ({np.sum(y_c_pred_original == 1)/len(y_c_pred_original)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb588115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for saving model artifacts\n",
    "import os\n",
    "save_dir = ''\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save the trained model\n",
    "model_path = os.path.join(save_dir, 'lstm(best)_model.h5')\n",
    "model.save(model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# 2. Save the scaler\n",
    "scaler_path = os.path.join(save_dir, 'lstm(best)_scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# 4. Save feature lists and model configuration\n",
    "config = {\n",
    "    'features': features,\n",
    "    'numerical_features_to_scale': numerical_features_to_scale,\n",
    "    'all_binary_features': all_binary_features,\n",
    "    'all_numerical_features': all_numerical_features,\n",
    "    'seq_length': seq_length,\n",
    "    'n_features': n_features,\n",
    "    'target': target,\n",
    "    'original_features': original_features,\n",
    "    'temporal_features': temporal_features,\n",
    "    'time_features': time_features,\n",
    "    'power_pattern_features': power_pattern_features,\n",
    "    'power_deviation_features': power_deviation_features,\n",
    "    'cyclical_features': cyclical_features,\n",
    "    'soft_mask_features': soft_mask_features,\n",
    "    'causal_delta_features': causal_delta_features,\n",
    "    'residual_features': residual_features,\n",
    "    'morning_evening_features': morning_evening_features,\n",
    "    'availability_features': availability_features,\n",
    "    'learnable_signal_features': learnable_signal_features\n",
    "}\n",
    "\n",
    "config_path = os.path.join(save_dir, 'lstm_model(best)_config.json')\n",
    "import json\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Model configuration saved to: {config_path}\")\n",
    "\n",
    "# 5. Save training history\n",
    "history_path = os.path.join(save_dir, 'training(best)_history.pkl')\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"Training history saved to: {history_path}\")\n",
    "\n",
    "print(f\"\\nAll model artifacts saved in directory: {save_dir}\")\n",
    "print(\"Contents:\")\n",
    "for file in os.listdir(save_dir):\n",
    "    print(f\"  - {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
