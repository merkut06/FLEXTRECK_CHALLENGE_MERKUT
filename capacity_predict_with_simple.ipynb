{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Column names\n",
    "SITE_COL = \"Site\"\n",
    "TIME_COL = \"Timestamp_Local\"\n",
    "TEMP_COL = \"Dry_Bulb_Temperature_C\"\n",
    "GHI_COL = \"Global_Horizontal_Radiation_W/m2\"\n",
    "POWER_COL = \"Building_Power_kW\"\n",
    "FLAG_COL = \"Demand_Response_Flag\"\n",
    "CAPACITY_COL = \"Demand_Response_Capacity_kW\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_output_dir(path: str) -> None:\n",
    "    \"\"\"Creates a directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def load_data_with_flags(train_path: str, flags_csv_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load training data and submission CSV with predicted flags\n",
    "    \"\"\"\n",
    "    # Load original training data\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    train_df[TIME_COL] = pd.to_datetime(train_df[TIME_COL])\n",
    "    \n",
    "    # Load submission with predicted flags\n",
    "    flags_df = pd.read_csv(flags_csv_path)\n",
    "    flags_df[TIME_COL] = pd.to_datetime(flags_df[TIME_COL])\n",
    "    \n",
    "    print(f\"[INFO] Loaded training data: {train_df.shape}\")\n",
    "    print(f\"[INFO] Loaded flags data: {flags_df.shape}\")\n",
    "    \n",
    "    return train_df, flags_df\n",
    "\n",
    "def prepare_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare features using only the raw columns from the CSV\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    "    \n",
    "    # Convert numeric columns to proper types\n",
    "    df[TEMP_COL] = pd.to_numeric(df[TEMP_COL], errors='coerce')\n",
    "    df[GHI_COL] = pd.to_numeric(df[GHI_COL], errors='coerce')\n",
    "    df[POWER_COL] = pd.to_numeric(df[POWER_COL], errors='coerce')\n",
    "    \n",
    "    # Fill any NaN values with 0\n",
    "    df[TEMP_COL] = df[TEMP_COL].fillna(0)\n",
    "    df[GHI_COL] = df[GHI_COL].fillna(0)\n",
    "    df[POWER_COL] = df[POWER_COL].fillna(0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapacityPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for predicting demand response capacity magnitude\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_sizes: list = [128, 64, 32]):\n",
    "        super(CapacityPredictor, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer with positive constraint for magnitude\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        layers.append(nn.Softplus())  # Ensures positive output\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(X: np.ndarray, y: np.ndarray, batch_size: int = 512, shuffle: bool = True) -> DataLoader:\n",
    "    \"\"\"Create PyTorch DataLoader from numpy arrays\"\"\"\n",
    "    X_tensor = torch.FloatTensor(X)\n",
    "    y_tensor = torch.FloatTensor(y)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def train_capacity_model(X_train: np.ndarray, y_train: np.ndarray, \n",
    "                        X_val: np.ndarray, y_val: np.ndarray,\n",
    "                        epochs: int = 200, lr: float = 0.001, \n",
    "                        device: str = \"cpu\") -> CapacityPredictor:\n",
    "    \"\"\"\n",
    "    Train the capacity prediction neural network\n",
    "    \"\"\"\n",
    "    device = torch.device(device)\n",
    "    model = CapacityPredictor(X_train.shape[1]).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.SmoothL1Loss()  # Robust to outliers\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "    \n",
    "    # Data loaders\n",
    "    train_loader = create_data_loader(X_train, y_train, shuffle=True)\n",
    "    val_loader = create_data_loader(X_val, y_val, shuffle=False)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "    \n",
    "    print(\"[INFO] Starting training...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"[Epoch {epoch:3d}] Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"[INFO] Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"[INFO] Training completed. Best validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_capacity_pipeline(train_path: str, \n",
    "                            flags_csv_path: str,\n",
    "                            output_path: str,\n",
    "                            epochs: int = 200,\n",
    "                            device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Main pipeline for capacity prediction\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Starting Capacity Prediction Pipeline\")\n",
    "    print(f\"[INFO] Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    train_df, test_with_flags = load_data_with_flags(train_path, flags_csv_path)\n",
    "    \n",
    "    # Find capacity column in training data\n",
    "    capacity_cols = [\"Demand_Response_Capacity_kW\", \"Demand_Response_Capacity_KW\"]\n",
    "    capacity_col = None\n",
    "    for col in capacity_cols:\n",
    "        if col in train_df.columns:\n",
    "            capacity_col = col\n",
    "            break\n",
    "    \n",
    "    if capacity_col is None:\n",
    "        raise ValueError(f\"Capacity column not found. Expected one of: {capacity_cols}\")\n",
    "    \n",
    "    print(f\"[INFO] Using capacity column: {capacity_col}\")\n",
    "    \n",
    "    # Prepare features for training data\n",
    "    train_featured = prepare_features(train_df)\n",
    "    \n",
    "    # Filter training data to events only (|flag| = 1)\n",
    "    event_mask = train_featured[FLAG_COL].abs() == 1\n",
    "    train_events = train_featured[event_mask].copy()\n",
    "    \n",
    "    print(f\"[INFO] Training events found: {len(train_events)}\")\n",
    "    print(f\"[INFO] Flag distribution in events: {train_events[FLAG_COL].value_counts().to_dict()}\")\n",
    "    \n",
    "    if len(train_events) == 0:\n",
    "        raise ValueError(\"No training events found with |flag| = 1\")\n",
    "    \n",
    "    # Feature columns for model - only raw columns from CSV\n",
    "    feature_cols = [\n",
    "        TEMP_COL, GHI_COL, POWER_COL\n",
    "    ]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_events = train_events[feature_cols].fillna(0).values\n",
    "    y_events = train_events[capacity_col].abs().values  # Use absolute capacity as magnitude\n",
    "    \n",
    "    # Split for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_events, y_events, test_size=0.2, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    print(f\"[INFO] Training set: {X_train_scaled.shape}, Validation set: {X_val_scaled.shape}\")\n",
    "    print(f\"[INFO] Capacity stats - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "    \n",
    "    # Train model\n",
    "    model = train_capacity_model(\n",
    "        X_train_scaled, y_train, X_val_scaled, y_val,\n",
    "        epochs=epochs, device=device\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_val_tensor = torch.FloatTensor(X_val_scaled).to(torch.device(device))\n",
    "        y_pred_val = model(X_val_tensor).cpu().numpy()\n",
    "    \n",
    "    val_mse = mean_squared_error(y_val, y_pred_val)\n",
    "    val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "    print(f\"[INFO] Validation MSE: {val_mse:.6f}, MAE: {val_mae:.6f}\")\n",
    "    \n",
    "    # Prepare test data features\n",
    "    test_featured = prepare_features(test_with_flags)\n",
    "    \n",
    "    # Predict capacity for all test data\n",
    "    X_test = test_featured[feature_cols].fillna(0).values\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled).to(torch.device(device))\n",
    "        capacity_magnitude = model(X_test_tensor).cpu().numpy()\n",
    "    \n",
    "    # Apply sign based on flag and set capacity\n",
    "    flags = test_featured[FLAG_COL].values\n",
    "    capacity_pred = np.zeros(len(flags))\n",
    "    \n",
    "    # For non-zero flags, apply predicted magnitude with flag sign\n",
    "    non_zero_mask = flags != 0\n",
    "    capacity_pred[non_zero_mask] = np.sign(flags[non_zero_mask]) * capacity_magnitude[non_zero_mask]\n",
    "    \n",
    "    # Create final submission\n",
    "    submission = pd.DataFrame({\n",
    "        SITE_COL: test_featured[SITE_COL].values,\n",
    "        TIME_COL: test_featured[TIME_COL].dt.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        FLAG_COL: flags,\n",
    "        CAPACITY_COL: capacity_pred\n",
    "    })\n",
    "    \n",
    "    # Save submission\n",
    "    ensure_output_dir(os.path.dirname(output_path))\n",
    "    submission.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"[OK] Capacity predictions saved to: {output_path}\")\n",
    "    print(f\"[INFO] Submission shape: {submission.shape}\")\n",
    "    print(f\"[INFO] Capacity distribution:\")\n",
    "    print(f\"  - Zero capacity (flag=0): {np.sum(capacity_pred == 0)}\")\n",
    "    print(f\"  - Positive capacity (flag=1): {np.sum(capacity_pred > 0)}\")\n",
    "    print(f\"  - Negative capacity (flag=-1): {np.sum(capacity_pred < 0)}\")\n",
    "    print(f\"  - Capacity stats (non-zero): Mean={capacity_pred[capacity_pred!=0].mean():.3f}, Std={capacity_pred[capacity_pred!=0].std():.3f}\")\n",
    "    \n",
    "    return model, scaler, submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged file created!\n",
      "Shape: (105120, 7)\n",
      "Columns: ['Site', 'Timestamp_Local', 'Dry_Bulb_Temperature_C', 'Global_Horizontal_Radiation_W/m2', 'Building_Power_kW', 'Demand_Response_Flag_x', 'Demand_Response_Flag_y']\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/test-data-v0.2.csv\")\n",
    "submission = pd.read_csv(\"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/xgb_submission_optimized.csv\")\n",
    "\n",
    "# Convert timestamps\n",
    "test_data[\"Timestamp_Local\"] = pd.to_datetime(test_data[\"Timestamp_Local\"])\n",
    "submission[\"Timestamp_Local\"] = pd.to_datetime(submission[\"Timestamp_Local\"])\n",
    "\n",
    "# Merge on Site and Timestamp_Local\n",
    "merged_data = test_data.merge(\n",
    "    submission[[\"Site\", \"Timestamp_Local\", \"Demand_Response_Flag\"]], \n",
    "    on=[\"Site\", \"Timestamp_Local\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Save merged file\n",
    "merged_data.to_csv(\"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/test_optimized_xgboost_merged_data_with_flags.csv\", index=False)\n",
    "\n",
    "print(\"Merged file created!\")\n",
    "print(f\"Shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {list(merged_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=[\"Demand_Response_Flag_x\"], inplace=True)\n",
    "merged_data.rename(columns={\"Demand_Response_Flag_y\": \"Demand_Response_Flag\"}, inplace=True)\n",
    "merged_data.to_csv(\"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/33test_optimized_xgboost_merged_data_with_flags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv(\"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/33test_optimized_xgboost_merged_data_with_flags.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Capacity Prediction Pipeline\n",
      "[INFO] Using device: cpu\n",
      "[INFO] Loaded training data: (105120, 7)\n",
      "[INFO] Loaded flags data: (105120, 6)\n",
      "[INFO] Using capacity column: Demand_Response_Capacity_kW\n",
      "[INFO] Training events found: 3109\n",
      "[INFO] Flag distribution in events: {-1: 2262, 1: 847}\n",
      "[INFO] Training set: (2487, 3), Validation set: (622, 3)\n",
      "[INFO] Capacity stats - Mean: 12.29, Std: 22.08\n",
      "[INFO] Starting training...\n",
      "[Epoch   0] Train Loss: 11.448710, Val Loss: 12.078916\n",
      "[Epoch  20] Train Loss: 8.525351, Val Loss: 8.865434\n",
      "[Epoch  40] Train Loss: 8.081026, Val Loss: 8.277314\n",
      "[Epoch  60] Train Loss: 7.973694, Val Loss: 7.944939\n",
      "[Epoch  80] Train Loss: 7.752458, Val Loss: 7.717909\n",
      "[Epoch 100] Train Loss: 7.456670, Val Loss: 7.598409\n",
      "[Epoch 120] Train Loss: 7.440469, Val Loss: 7.555068\n",
      "[INFO] Early stopping at epoch 133\n",
      "[INFO] Training completed. Best validation loss: 7.491379\n",
      "[INFO] Validation MSE: 343.180018, MAE: 8.006022\n",
      "[OK] Capacity predictions saved to: /Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/capacity_predictions_nn4.csv\n",
      "[INFO] Submission shape: (105120, 4)\n",
      "[INFO] Capacity distribution:\n",
      "  - Zero capacity (flag=0): 98580\n",
      "  - Positive capacity (flag=1): 322\n",
      "  - Negative capacity (flag=-1): 6218\n",
      "  - Capacity stats (non-zero): Mean=-1.427, Std=1.225\n",
      "[DONE] Capacity prediction pipeline completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    TRAIN_PATH = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/training-data-v0.2.csv\"\n",
    "    \n",
    "    # You can use any of the generated submission files\n",
    "    FLAGS_CSV_PATH = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/33test_optimized_xgboost_merged_data_with_flags.csv\"\n",
    "    \n",
    "    OUTPUT_PATH = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/codes/phase_2_codes/outputs_simple/capacity_predictions_nn4.csv\"\n",
    "    \n",
    "    # Run pipeline\n",
    "    model, scaler, submission = predict_capacity_pipeline(\n",
    "        train_path=TRAIN_PATH,\n",
    "        flags_csv_path=FLAGS_CSV_PATH,\n",
    "        output_path=OUTPUT_PATH,\n",
    "        epochs=200,\n",
    "        device=\"cpu\"  # Change to \"cuda\" if you have GPU\n",
    "    )\n",
    "    \n",
    "    print(\"[DONE] Capacity prediction pipeline completed!\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
