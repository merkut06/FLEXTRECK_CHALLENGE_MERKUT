{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute paths (as you provided)\n",
    "TRAIN_V02 = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/training-data-v0.2.csv\"\n",
    "TEST_V02  = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/test-data-v0.2.csv\"\n",
    "\n",
    "OUT_TRAIN = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/train_v0.2_simple.csv\"\n",
    "OUT_TEST  = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/test_v0.2_simple.csv\"\n",
    "\n",
    "POWER_COL = \"Building_Power_kW\"\n",
    "TEMP_COL  = \"Dry_Bulb_Temperature_C\"\n",
    "GHI_COL   = \"Global_Horizontal_Radiation_W/m2\"\n",
    "SITE_COL  = \"Site\"\n",
    "TIME_COL  = \"Timestamp_Local\"\n",
    "FLAG_COL  = \"Demand_Response_Flag\"  # may be partly null in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "     df = df.copy()\n",
    "     df[TIME_COL] = pd.to_datetime(df[TIME_COL])\n",
    " \n",
    "     df[\"weekday\"] = df[TIME_COL].dt.weekday.astype(int)                 # Mon=0 .. Sun=6\n",
    "     df[\"week_of_year\"] = df[TIME_COL].dt.isocalendar().week.astype(int)\n",
    "     df[\"hour\"] = df[TIME_COL].dt.hour.astype(int)\n",
    "     df[\"month\"] = df[TIME_COL].dt.month.astype(int)\n",
    " \n",
    "     # Parts of day to match your prior columns\n",
    "     df[\"part_Morning\"]   = ((df[\"hour\"] >= 6)  & (df[\"hour\"] < 12)).astype(int)\n",
    "     df[\"part_Afternoon\"] = ((df[\"hour\"] >= 12) & (df[\"hour\"] < 18)).astype(int)\n",
    " \n",
    "     # is_saturday like in your previous CSV\n",
    "     df[\"is_saturday\"] = (df[\"weekday\"] == 5).astype(int)\n",
    "\n",
    "     # Simple seasonal indicators with names matching your prior columns\n",
    "     df[\"intensity_Winter_1\"] = df[\"month\"].isin([12, 1, 2]).astype(int)\n",
    "     df[\"intensity_Summer_0\"] = df[\"month\"].isin([6, 7, 8]).astype(int)\n",
    " \n",
    "     # Active hours (typical working hours 08:00–20:00); adjust if you used a different range\n",
    "     df[\"active_hours\"] = ((df[\"hour\"] >= 8) & (df[\"hour\"] <= 20)).astype(int)\n",
    " \n",
    "     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_power_lag_roll_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Ensure time ordering per site\n",
    "    df = df.sort_values([SITE_COL, TIME_COL])\n",
    " \n",
    "    def _per_site(g: pd.DataFrame) -> pd.DataFrame:\n",
    "        g = g.copy()\n",
    "        # Lags\n",
    "        g[\"Power_lag_1\"] = g[POWER_COL].shift(1)\n",
    "        g[\"Power_lag_2\"] = g[POWER_COL].shift(2)\n",
    "        g[\"Power_lag_4\"] = g[POWER_COL].shift(4)\n",
    " \n",
    "        # Fill initial lag NaNs with current power (this matches the look of your prior CSV head)\n",
    "        for c in [\"Power_lag_1\", \"Power_lag_2\", \"Power_lag_4\"]:\n",
    "            g[c] = g[c].fillna(g[POWER_COL])\n",
    " \n",
    "        # Rolling mean (window=3, include current; min_periods=1 to avoid NaNs)\n",
    "        g[\"power_roll_mean_3\"] = (\n",
    "            g[POWER_COL].rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    " \n",
    "#         # First difference\n",
    "        g[\"power_diff_1\"] = g[POWER_COL].diff(1).fillna(0.0)\n",
    " \n",
    "        return g\n",
    " \n",
    "        df = df.groupby(SITE_COL, group_keys=False).apply(_per_site)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = add_time_features(df)\n",
    "    df = add_power_lag_roll_features(df)\n",
    "    df = df.drop(columns=[\"hour\", \"month\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1t/15hmc_h12xnbygjnmf7kj43h0000gn/T/ipykernel_10718/480967996.py:68: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(SITE_COL, group_keys=False).apply(_per_site)\n",
      "/var/folders/1t/15hmc_h12xnbygjnmf7kj43h0000gn/T/ipykernel_10718/480967996.py:68: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(SITE_COL, group_keys=False).apply(_per_site)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote: /Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/train_v0.2_fe.csv  (shape=(105120, 20))\n",
      "[OK] Wrote: /Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/test_v0.2_fe.csv   (shape=(105120, 19))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Read\n",
    "    train = pd.read_csv(TRAIN_V02)\n",
    "    test  = pd.read_csv(TEST_V02)\n",
    "\n",
    "    # Build features\n",
    "    train_fe = build_features(train)\n",
    "    test_fe  = build_features(test)\n",
    "\n",
    "    # Ensure consistent column order: IDs first, then features, then targets if present\n",
    "    id_cols = [SITE_COL, TIME_COL]\n",
    "    base_cols = [TEMP_COL, GHI_COL, POWER_COL]\n",
    "    target_cols = [c for c in [FLAG_COL, \"Demand_Response_Capacity_KW\", \"Demand_Response_Capacity_kW\"] if c in train_fe.columns]\n",
    "\n",
    "    # Move columns into a tidy order for train\n",
    "    feat_cols_train = [c for c in train_fe.columns if c not in id_cols + target_cols]\n",
    "    train_fe = train_fe[id_cols + feat_cols_train + target_cols]\n",
    "\n",
    "    # For test, there is no capacity column; keep FLAG if present\n",
    "    target_cols_test = [c for c in [FLAG_COL] if c in test_fe.columns]\n",
    "    feat_cols_test = [c for c in test_fe.columns if c not in id_cols + target_cols_test]\n",
    "    test_fe = test_fe[id_cols + feat_cols_test + target_cols_test]\n",
    "\n",
    "    # Write\n",
    "    train_fe.to_csv(OUT_TRAIN, index=False)\n",
    "    test_fe.to_csv(OUT_TEST, index=False)\n",
    "\n",
    "    print(f\"[OK] Wrote: {OUT_TRAIN}  (shape={train_fe.shape})\")\n",
    "    print(f\"[OK] Wrote: {OUT_TEST}   (shape={test_fe.shape})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazy imports for optional libs\n",
    "def _import_xgboost():\n",
    "    try:\n",
    "        from xgboost import XGBClassifier\n",
    "        return XGBClassifier\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"xgboost is not installed. Please run: pip install xgboost\") from e\n",
    "\n",
    "def _import_lightgbm():\n",
    "    try:\n",
    "        from lightgbm import LGBMClassifier\n",
    "        return LGBMClassifier\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"lightgbm is not installed. Please run: pip install lightgbm\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_COL = \"Demand_Response_Flag\"\n",
    "DROP_FEATURES = [\"Demand_Response_Flag\"]\n",
    "\n",
    "def ensure_output_dir(outdir: str):\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "def load_data(train_path: str, test_path: str):\n",
    "    train = pd.read_csv(train_path)\n",
    "    test = pd.read_csv(test_path)\n",
    "    if TARGET_COL not in train.columns:\n",
    "        raise ValueError(f\"Target '{TARGET_COL}' not found in train CSV.\")\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(train: pd.DataFrame, test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Prepares training features (drops target + non-numeric cols).\n",
    "    \"\"\"\n",
    "    # Target and features\n",
    "    y = train[TARGET_COL].copy()\n",
    "    X = train.drop(columns=[c for c in DROP_FEATURES if c in train.columns], errors=\"ignore\").copy()\n",
    "\n",
    "    # Drop object-type columns (e.g., Site, Timestamp_Local)\n",
    "    obj_cols = list(X.select_dtypes(include=[\"object\"]).columns)\n",
    "    if obj_cols:\n",
    "        print(f\"[INFO] Dropping object columns: {obj_cols}\")\n",
    "        X = X.drop(columns=obj_cols)\n",
    "\n",
    "    # Info\n",
    "    print(f\"[INFO] Train shape: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"[INFO] Class distribution (train):\\n{y.value_counts(normalize=True).sort_index()}\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chrono_split(df, target_col=\"Demand_Response_Flag\", time_col=\"Timestamp_Local\", val_size=0.2, embargo_hours=0):\n",
    "    df = df.copy()\n",
    "    df[time_col] = pd.to_datetime(df[time_col])\n",
    "    df = df.sort_values(time_col).reset_index(drop=True)\n",
    "\n",
    "    # Split index\n",
    "    cutoff_idx = int(len(df) * (1 - val_size))\n",
    "    \n",
    "    if embargo_hours > 0:\n",
    "        cutoff_time = df.loc[cutoff_idx, time_col]\n",
    "        train = df[df[time_col] < (cutoff_time - pd.Timedelta(hours=embargo_hours))]\n",
    "        val   = df[df[time_col] >= (cutoff_time + pd.Timedelta(hours=embargo_hours))]\n",
    "    else:\n",
    "        train = df.iloc[:cutoff_idx]\n",
    "        val   = df.iloc[cutoff_idx:]\n",
    "    \n",
    "    # Separate features/target\n",
    "    X_train, y_train = train.drop(columns=[target_col]), train[target_col]\n",
    "    X_val, y_val     = val.drop(columns=[target_col]), val[target_col]\n",
    "\n",
    "    # Info\n",
    "    print(\"⏳ Chronological split:\")\n",
    "    print(f\"Train size: {len(train)}, Validation size: {len(val)}\")\n",
    "    print(\"Train class distribution:\\n\", y_train.value_counts(normalize=True).round(3))\n",
    "    print(\"Validation class distribution:\\n\", y_val.value_counts(normalize=True).round(3))\n",
    "\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advanced_class_weights(y_train, strategy='extreme'):\n",
    "    \"\"\"\n",
    "    Compute advanced class weights using different strategies\n",
    "    \"\"\"\n",
    "    values, counts = np.unique(y_train, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    \n",
    "    if strategy == 'balanced':\n",
    "        # Standard balanced weights\n",
    "        class_weights = {int(v): float(total / (len(values) * c)) for v, c in zip(values, counts)}\n",
    "    \n",
    "    elif strategy == 'extreme':\n",
    "        # More aggressive weights for extremely imbalanced data\n",
    "        base_weights = {int(v): float(total / (len(values) * c)) for v, c in zip(values, counts)}\n",
    "        # Apply additional multipliers for minority classes\n",
    "        multipliers = {}\n",
    "        for v, c in zip(values, counts):\n",
    "            if c < total * 0.01:  # Less than 1% of data\n",
    "                multipliers[int(v)] = 3.0\n",
    "            elif c < total * 0.05:  # Less than 5% of data\n",
    "                multipliers[int(v)] = 2.0\n",
    "            else:\n",
    "                multipliers[int(v)] = 0.5  # Reduce majority class weight\n",
    "        \n",
    "        class_weights = {k: v * multipliers[k] for k, v in base_weights.items()}\n",
    "    \n",
    "    elif strategy == 'custom':\n",
    "        # Custom strategy based on domain knowledge\n",
    "        minority_penalty = 50.0\n",
    "        majority_penalty = 0.3\n",
    "        \n",
    "        class_weights = {}\n",
    "        for v, c in zip(values, counts):\n",
    "            if c < 500:  # Very rare class\n",
    "                class_weights[int(v)] = minority_penalty\n",
    "            elif c > total * 0.8:  # Dominant class  \n",
    "                class_weights[int(v)] = majority_penalty\n",
    "            else:\n",
    "                class_weights[int(v)] = 1.0\n",
    "    \n",
    "    print(f\"[INFO] Advanced class weights ({strategy}):\", class_weights)\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_thresholds(y_true, y_proba, class_labels=[0, 1, 2], metric='f1'):\n",
    "    \"\"\"\n",
    "    Find optimal thresholds for each class using different metrics\n",
    "    \"\"\"\n",
    "    optimal_thresholds = {}\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        # Convert to binary problem for this class\n",
    "        y_binary = (y_true == class_label).astype(int)\n",
    "        y_scores = y_proba[:, i]\n",
    "        \n",
    "        if metric == 'f1':\n",
    "            precision, recall, thresholds = precision_recall_curve(y_binary, y_scores)\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            optimal_idx = np.argmax(f1_scores)\n",
    "            optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "            \n",
    "        elif metric == 'precision_recall_balance':\n",
    "            precision, recall, thresholds = precision_recall_curve(y_binary, y_scores)\n",
    "            # Find threshold where precision ≈ recall\n",
    "            pr_diff = np.abs(precision[:-1] - recall[:-1])\n",
    "            optimal_idx = np.argmin(pr_diff)\n",
    "            optimal_threshold = thresholds[optimal_idx]\n",
    "            \n",
    "        else:  # Default to 0.5\n",
    "            optimal_threshold = 0.5\n",
    "            \n",
    "        optimal_thresholds[class_label] = optimal_threshold\n",
    "        \n",
    "        print(f\"[INFO] Optimal threshold for class {class_label}: {optimal_threshold:.4f}\")\n",
    "    \n",
    "    return optimal_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_optimal_thresholds(y_proba, thresholds, class_labels=[0, 1, 2]):\n",
    "    \"\"\"\n",
    "    Apply optimal thresholds to get predictions\n",
    "    \"\"\"\n",
    "    n_samples = y_proba.shape[0]\n",
    "    y_pred = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        threshold = thresholds[class_label]\n",
    "        mask = y_proba[:, i] >= threshold\n",
    "        y_pred[mask] = class_label\n",
    "    \n",
    "    # Handle cases where no class meets threshold or multiple classes meet threshold\n",
    "    # Use argmax as fallback\n",
    "    no_prediction_mask = np.sum([y_proba[:, i] >= thresholds[class_labels[i]] \n",
    "                                for i in range(len(class_labels))], axis=0) == 0\n",
    "    \n",
    "    if np.any(no_prediction_mask):\n",
    "        y_pred[no_prediction_mask] = np.argmax(y_proba[no_prediction_mask], axis=1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(X_train, y_train, model_type='xgb', class_weights=None, \n",
    "                               sample_weight=None, cv_folds=3, n_trials=20):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization using RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    print(f\"[INFO] Starting hyperparameter optimization for {model_type}...\")\n",
    "    \n",
    "    if model_type == 'xgb':\n",
    "        XGBClassifier = _import_xgboost()\n",
    "        \n",
    "        # Parameter grid for XGBoost\n",
    "        param_grid = {\n",
    "            'n_estimators': [300, 500, 800, 1200],\n",
    "            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5, 6, 7],\n",
    "            'min_child_weight': [1, 3, 5, 7],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_lambda': [0.1, 0.5, 1.0, 2.0],\n",
    "            'reg_alpha': [0.0, 0.1, 0.5, 1.0]\n",
    "        }\n",
    "        \n",
    "        base_estimator = XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            eval_metric='mlogloss',\n",
    "            tree_method='hist',\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Custom scoring for imbalanced data\n",
    "        scoring = 'f1_macro'\n",
    "        \n",
    "    elif model_type == 'lgbm':\n",
    "        LGBMClassifier = _import_lightgbm()\n",
    "        \n",
    "        # Parameter grid for LightGBM\n",
    "        param_grid = {\n",
    "            'n_estimators': [300, 500, 800, 1200],\n",
    "            'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "            'max_depth': [-1, 3, 5, 7],\n",
    "            'num_leaves': [15, 31, 63, 127],\n",
    "            'min_child_samples': [20, 50, 100],\n",
    "            'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "            'reg_lambda': [0.1, 0.5, 1.0, 2.0],\n",
    "            'reg_alpha': [0.0, 0.1, 0.5, 1.0]\n",
    "        }\n",
    "        \n",
    "        base_params = {\n",
    "            'objective': 'multiclass',\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            base_params['class_weight'] = class_weights\n",
    "        else:\n",
    "            base_params['is_unbalance'] = True\n",
    "            \n",
    "        base_estimator = LGBMClassifier(**base_params)\n",
    "        scoring = 'f1_macro'\n",
    "    \n",
    "    # Perform randomized search\n",
    "    random_search = RandomizedSearchCV(\n",
    "        base_estimator,\n",
    "        param_grid,\n",
    "        n_iter=n_trials,\n",
    "        cv=cv_folds,\n",
    "        scoring=scoring,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit with sample weights if provided (XGBoost)\n",
    "    if model_type == 'xgb' and sample_weight is not None:\n",
    "        random_search.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    else:\n",
    "        random_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"[INFO] Best {model_type} parameters:\", random_search.best_params_)\n",
    "    print(f\"[INFO] Best {model_type} CV score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    return random_search.best_estimator_, random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimized_xgb(X_train, y_train, sample_weight=None, best_params=None):\n",
    "    \"\"\"Train XGBoost with optimized parameters\"\"\"\n",
    "    XGBClassifier = _import_xgboost()\n",
    "    \n",
    "    if best_params is None:\n",
    "        # Default optimized parameters based on typical energy data patterns\n",
    "        params = {\n",
    "            'n_estimators': 800,\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': 5,\n",
    "            'min_child_weight': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_lambda': 1.0,\n",
    "            'reg_alpha': 0.1,\n",
    "            'objective': 'multi:softprob',\n",
    "            'eval_metric': 'mlogloss',\n",
    "            'tree_method': 'hist',\n",
    "            'random_state': 42\n",
    "        }\n",
    "    else:\n",
    "        params = best_params.copy()\n",
    "        params.update({\n",
    "            'objective': 'multi:softprob',\n",
    "            'eval_metric': 'mlogloss', \n",
    "            'tree_method': 'hist',\n",
    "            'random_state': 42\n",
    "        })\n",
    "    \n",
    "    xgb = XGBClassifier(**params)\n",
    "    xgb.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    return xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_optimized_lgbm(X_train, y_train, class_weights=None, best_params=None):\n",
    "    \"\"\"Train LightGBM with optimized parameters\"\"\"\n",
    "    LGBMClassifier = _import_lightgbm()\n",
    "    \n",
    "    if best_params is None:\n",
    "        # Default optimized parameters\n",
    "        params = {\n",
    "            'n_estimators': 1000,\n",
    "            'learning_rate': 0.03,\n",
    "            'max_depth': -1,\n",
    "            'num_leaves': 31,\n",
    "            'min_child_samples': 50,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_lambda': 1.0,\n",
    "            'reg_alpha': 0.1,\n",
    "            'objective': 'multiclass',\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        }\n",
    "    else:\n",
    "        params = best_params.copy()\n",
    "        params.update({\n",
    "            'objective': 'multiclass',\n",
    "            'random_state': 42,\n",
    "            'verbose': -1\n",
    "        })\n",
    "    \n",
    "    if class_weights is not None:\n",
    "        params['class_weight'] = class_weights\n",
    "    else:\n",
    "        params['is_unbalance'] = True\n",
    "    \n",
    "    lgbm = LGBMClassifier(**params)\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    return lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_and_report(y_true, y_pred, y_proba, model_name, outdir, class_labels=[0,1,2], \n",
    "                   thresholds=None, y_pred_thresh=None):\n",
    "    \"\"\"Enhanced evaluation with threshold optimization results\"\"\"\n",
    "    \n",
    "    # Standard metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # ROC-AUC (macro)\n",
    "    try:\n",
    "        y_bin = label_binarize(y_true, classes=class_labels)\n",
    "        auc_macro = roc_auc_score(y_bin, y_proba, multi_class=\"ovr\", average=\"macro\")\n",
    "    except Exception:\n",
    "        auc_macro = np.nan\n",
    "\n",
    "    print(f\"\\n=== {model_name} Validation Metrics (Default Threshold) ===\")\n",
    "    print(f\"Accuracy        : {acc:.4f}\")\n",
    "    print(f\"Macro Precision : {prec:.4f}\")\n",
    "    print(f\"Macro Recall    : {rec:.4f}\")\n",
    "    print(f\"Macro F1-score  : {f1:.4f}\")\n",
    "    print(f\"Macro ROC-AUC   : {auc_macro if not np.isnan(auc_macro) else 'N/A'}\")\n",
    "    \n",
    "    # Threshold optimized metrics\n",
    "    acc_thresh = None\n",
    "    f1_thresh = None\n",
    "    if y_pred_thresh is not None:\n",
    "        acc_thresh = accuracy_score(y_true, y_pred_thresh)\n",
    "        prec_thresh = precision_score(y_true, y_pred_thresh, average=\"macro\", zero_division=0)\n",
    "        rec_thresh  = recall_score(y_true, y_pred_thresh, average=\"macro\", zero_division=0)\n",
    "        f1_thresh   = f1_score(y_true, y_pred_thresh, average=\"macro\", zero_division=0)\n",
    "        \n",
    "        print(f\"\\n=== {model_name} Validation Metrics (Optimized Threshold) ===\")\n",
    "        print(f\"Accuracy        : {acc_thresh:.4f} (Δ: {acc_thresh-acc:+.4f})\")\n",
    "        print(f\"Macro Precision : {prec_thresh:.4f} (Δ: {prec_thresh-prec:+.4f})\")\n",
    "        print(f\"Macro Recall    : {rec_thresh:.4f} (Δ: {rec_thresh-rec:+.4f})\")\n",
    "        print(f\"Macro F1-score  : {f1_thresh:.4f} (Δ: {f1_thresh-f1:+.4f})\")\n",
    "\n",
    "    print(\"\\nClassification report (Default):\\n\",\n",
    "          classification_report(y_true, y_pred, digits=4, zero_division=0))\n",
    "    \n",
    "    if y_pred_thresh is not None:\n",
    "        print(\"\\nClassification report (Optimized):\\n\",\n",
    "              classification_report(y_true, y_pred_thresh, digits=4, zero_division=0))\n",
    "\n",
    "    # Plot confusion matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Default threshold confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f\"{model_name} - Default Threshold\")\n",
    "    axes[0].set_ylabel('True label')\n",
    "    axes[0].set_xlabel('Predicted label')\n",
    "    \n",
    "    # Optimized threshold confusion matrix\n",
    "    if y_pred_thresh is not None:\n",
    "        cm_thresh = confusion_matrix(y_true, y_pred_thresh, labels=class_labels)\n",
    "        sns.heatmap(cm_thresh, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "        axes[1].set_title(f\"{model_name} - Optimized Threshold\")\n",
    "        axes[1].set_ylabel('True label')\n",
    "        axes[1].set_xlabel('Predicted label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    cm_path = os.path.join(outdir, f\"{model_name.replace(' ', '_').lower()}_confusion_matrices.png\")\n",
    "    plt.savefig(cm_path, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": prec,\n",
    "        \"recall_macro\": rec,\n",
    "        \"f1_macro\": f1,\n",
    "        \"roc_auc_macro\": auc_macro,\n",
    "        \"accuracy_thresh\": acc_thresh,\n",
    "        \"f1_macro_thresh\": f1_thresh,\n",
    "        \"cm_path\": cm_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, X_test, outdir, name_prefix, idx_to_label=None, \n",
    "                    optimal_thresholds=None, class_labels=[0, 1, 2]):\n",
    "    \"\"\"Enhanced prediction with threshold optimization\"\"\"\n",
    "    # Define feature columns (exclude ID cols)\n",
    "    feature_cols = [c for c in X_test.columns if c not in [\"Site\", \"Timestamp_Local\"]]\n",
    "    \n",
    "    # Predictions + probabilities\n",
    "    proba = model.predict_proba(X_test[feature_cols])\n",
    "    pred_default = model.predict(X_test[feature_cols])\n",
    "    \n",
    "    # Apply optimal thresholds if available\n",
    "    pred_optimized = None\n",
    "    if optimal_thresholds is not None:\n",
    "        pred_optimized = apply_optimal_thresholds(proba, optimal_thresholds, class_labels)\n",
    "    \n",
    "    # Map back to original labels if needed\n",
    "    def map_predictions(pred_array):\n",
    "        if idx_to_label and pred_array is not None:\n",
    "            return [idx_to_label.get(i, i) for i in pred_array]\n",
    "        return pred_array\n",
    "    \n",
    "    pred_default_mapped = map_predictions(pred_default)\n",
    "    pred_optimized_mapped = map_predictions(pred_optimized) if pred_optimized is not None else None\n",
    "\n",
    "    # --- 1) Official submission (default threshold) ---\n",
    "    df_official = pd.DataFrame({\n",
    "        \"Site\": X_test[\"Site\"].values,\n",
    "        \"Timestamp_Local\": X_test[\"Timestamp_Local\"].values,\n",
    "        \"Demand_Response_Flag\": pred_default_mapped\n",
    "    })\n",
    "    out_path_official = os.path.join(outdir, f\"{name_prefix}_submission_default.csv\")\n",
    "    df_official.to_csv(out_path_official, index=False)\n",
    "    print(f\"[INFO] Wrote official submission (default) → {out_path_official}\")\n",
    "\n",
    "    # --- 2) Official submission (optimized threshold) ---\n",
    "    if pred_optimized_mapped is not None:\n",
    "        df_official_opt = pd.DataFrame({\n",
    "            \"Site\": X_test[\"Site\"].values,\n",
    "            \"Timestamp_Local\": X_test[\"Timestamp_Local\"].values,\n",
    "            \"Demand_Response_Flag\": pred_optimized_mapped\n",
    "        })\n",
    "        out_path_official_opt = os.path.join(outdir, f\"{name_prefix}_submission_optimized.csv\")\n",
    "        df_official_opt.to_csv(out_path_official_opt, index=False)\n",
    "        print(f\"[INFO] Wrote official submission (optimized) → {out_path_official_opt}\")\n",
    "\n",
    "    # --- 3) Debug submission (with probabilities) ---\n",
    "    df_debug = df_official.copy()\n",
    "    if pred_optimized_mapped is not None:\n",
    "        df_debug[\"Demand_Response_Flag_Optimized\"] = pred_optimized_mapped\n",
    "    \n",
    "    for class_index in range(proba.shape[1]):\n",
    "        label_name = idx_to_label[class_index] if idx_to_label else class_index\n",
    "        df_debug[f\"prob_{label_name}\"] = proba[:, class_index]\n",
    "    \n",
    "    out_path_debug = os.path.join(outdir, f\"{name_prefix}_submission_with_probs.csv\")\n",
    "    df_debug.to_csv(out_path_debug, index=False)\n",
    "    print(f\"[INFO] Wrote debug submission → {out_path_debug}\")\n",
    "\n",
    "    return out_path_official, out_path_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_path=None, test_path=None, outdir=\"outputs_advanced\", val_size=0.2, \n",
    "         weight_strategy=\"extreme\", optimize_hyperparams=True, n_trials=20):\n",
    "    \"\"\"\n",
    "    Advanced Energy Classification Pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default paths if not provided\n",
    "    if train_path is None:\n",
    "        train_path = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/training-data-v0.2.csv\"\n",
    "    if test_path is None:\n",
    "        test_path = \"/Users/ibrahimyucel/Downloads/ULUSLARARASI_ENERJI_YARISMASI/data2/test-data-v0.2.csv\"\n",
    "    \n",
    "    ensure_output_dir(outdir)\n",
    "    print(f\"[INFO] Advanced Energy Classification Pipeline Started\")\n",
    "    print(f\"[INFO] Weight strategy: {weight_strategy}\")\n",
    "    print(f\"[INFO] Hyperparameter optimization: {optimize_hyperparams}\")\n",
    "\n",
    "    # --- Load data ---\n",
    "    train, test = load_data(train_path, test_path)\n",
    "    X, y = prepare_features(train, test)\n",
    "\n",
    "    # --- Label mapping: {-1,0,1} → {0,1,2} ---\n",
    "    label_to_idx = {-1: 0, 0: 1, 1: 2}\n",
    "    idx_to_label = {v: k for k, v in label_to_idx.items()}\n",
    "    train[TARGET_COL] = train[TARGET_COL].map(label_to_idx).astype(int)\n",
    "\n",
    "    # --- Chronological split ---\n",
    "    X_tr, y_tr, X_val, y_val = chrono_split(\n",
    "        train,\n",
    "        target_col=TARGET_COL,\n",
    "        time_col=\"Timestamp_Local\",\n",
    "        val_size=val_size,\n",
    "        embargo_hours=0\n",
    "    )\n",
    "\n",
    "    # Drop non-feature ID cols for training\n",
    "    feature_cols = [c for c in X_tr.columns if c not in [\"Site\", \"Timestamp_Local\"]]\n",
    "    X_tr_features = X_tr[feature_cols]\n",
    "    X_val_features = X_val[feature_cols]\n",
    "\n",
    "    # --- Advanced Class Weights ---\n",
    "    classes = np.unique(y_tr)\n",
    "    cw_advanced = compute_advanced_class_weights(y_tr, strategy=weight_strategy)\n",
    "    \n",
    "    # Standard balanced weights for comparison\n",
    "    weights_standard = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_tr)\n",
    "    cw_standard = dict(zip(classes, weights_standard))\n",
    "    print(\"[INFO] Standard balanced weights:\", cw_standard)\n",
    "    \n",
    "    # Sample weights for XGBoost\n",
    "    sample_w_advanced = y_tr.map(cw_advanced).values\n",
    "\n",
    "    # --- Hyperparameter Optimization ---\n",
    "    if optimize_hyperparams:\n",
    "        print(\"\\n[INFO] Performing hyperparameter optimization...\")\n",
    "        \n",
    "        # Optimize XGBoost\n",
    "        xgb_best, xgb_best_params = hyperparameter_optimization(\n",
    "            X_tr_features, y_tr, model_type='xgb', \n",
    "            sample_weight=sample_w_advanced, n_trials=n_trials\n",
    "        )\n",
    "        \n",
    "        # Optimize LightGBM\n",
    "        lgbm_best, lgbm_best_params = hyperparameter_optimization(\n",
    "            X_tr_features, y_tr, model_type='lgbm', \n",
    "            class_weights=cw_advanced, n_trials=n_trials\n",
    "        )\n",
    "        \n",
    "        xgb_model = xgb_best\n",
    "        lgbm_model = lgbm_best\n",
    "    else:\n",
    "        print(\"\\n[INFO] Using default optimized parameters...\")\n",
    "        xgb_model = train_optimized_xgb(X_tr_features, y_tr, sample_weight=sample_w_advanced)\n",
    "        lgbm_model = train_optimized_lgbm(X_tr_features, y_tr, class_weights=cw_advanced)\n",
    "\n",
    "    # --- Validation Predictions ---\n",
    "    print(\"\\n[INFO] Getting validation predictions...\")\n",
    "    xgb_proba_val = xgb_model.predict_proba(X_val_features)\n",
    "    lgbm_proba_val = lgbm_model.predict_proba(X_val_features)\n",
    "\n",
    "    xgb_pred_val = np.argmax(xgb_proba_val, axis=1)\n",
    "    lgbm_pred_val = np.argmax(lgbm_proba_val, axis=1)\n",
    "\n",
    "    # --- Threshold Optimization ---\n",
    "    print(\"\\n[INFO] Optimizing thresholds...\")\n",
    "    xgb_thresholds = find_optimal_thresholds(y_val, xgb_proba_val, class_labels=[0, 1, 2], metric='f1')\n",
    "    lgbm_thresholds = find_optimal_thresholds(y_val, lgbm_proba_val, class_labels=[0, 1, 2], metric='f1')\n",
    "\n",
    "    # Apply optimized thresholds\n",
    "    xgb_pred_val_thresh = apply_optimal_thresholds(xgb_proba_val, xgb_thresholds, class_labels=[0, 1, 2])\n",
    "    lgbm_pred_val_thresh = apply_optimal_thresholds(lgbm_proba_val, lgbm_thresholds, class_labels=[0, 1, 2])\n",
    "\n",
    "    # --- Evaluate models ---\n",
    "    print(\"\\n[INFO] Evaluating models...\")\n",
    "    xgb_metrics = eval_and_report(y_val, xgb_pred_val, xgb_proba_val, \"XGBoost_Advanced\", \n",
    "                                  outdir, class_labels=[0, 1, 2], \n",
    "                                  thresholds=xgb_thresholds, y_pred_thresh=xgb_pred_val_thresh)\n",
    "    \n",
    "    lgbm_metrics = eval_and_report(y_val, lgbm_pred_val, lgbm_proba_val, \"LightGBM_Advanced\", \n",
    "                                   outdir, class_labels=[0, 1, 2],\n",
    "                                   thresholds=lgbm_thresholds, y_pred_thresh=lgbm_pred_val_thresh)\n",
    "\n",
    "    # --- Save summary ---\n",
    "    summary_data = []\n",
    "    summary_data.append({\"model\": \"XGBoost_Advanced\", \"threshold_type\": \"default\", **xgb_metrics})\n",
    "    summary_data.append({\"model\": \"LightGBM_Advanced\", \"threshold_type\": \"default\", **lgbm_metrics})\n",
    "    \n",
    "    summary = pd.DataFrame(summary_data)\n",
    "    summary_path = os.path.join(outdir, \"advanced_validation_metrics_summary.csv\")\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    print(f\"[INFO] Wrote validation metrics summary → {summary_path}\")\n",
    "\n",
    "    # --- Test Predictions ---\n",
    "    print(\"\\n[INFO] Preparing test predictions...\")\n",
    "    id_cols = [\"Site\", \"Timestamp_Local\"] \n",
    "    feat_cols = list(X_tr_features.columns)\n",
    "    X_test_for_pred = test.copy()\n",
    "\n",
    "    # Align features\n",
    "    missing = [c for c in feat_cols if c not in X_test_for_pred.columns]\n",
    "    for c in missing:\n",
    "        X_test_for_pred[c] = 0\n",
    "\n",
    "    extra = [c for c in X_test_for_pred.columns if c not in (feat_cols + id_cols)]\n",
    "    if extra:\n",
    "        X_test_for_pred = X_test_for_pred.drop(columns=extra)\n",
    "\n",
    "    X_test_for_pred = X_test_for_pred[id_cols + feat_cols]\n",
    "\n",
    "    # Make predictions\n",
    "    predict_and_save(xgb_model, X_test_for_pred, outdir, \"xgb_advanced\", \n",
    "                     idx_to_label, xgb_thresholds, class_labels=[0, 1, 2])\n",
    "    \n",
    "    predict_and_save(lgbm_model, X_test_for_pred, outdir, \"lgbm_advanced\", \n",
    "                     idx_to_label, lgbm_thresholds, class_labels=[0, 1, 2])\n",
    "\n",
    "    # Save additional outputs\n",
    "    feats_path = os.path.join(outdir, \"features_used.txt\")\n",
    "    with open(feats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for c in feat_cols:\n",
    "            f.write(f\"{c}\\n\")\n",
    "    print(f\"[INFO] Feature list saved → {feats_path}\")\n",
    "\n",
    "    print(f\"\\n[DONE] Pipeline completed! All outputs in: {os.path.abspath(outdir)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
